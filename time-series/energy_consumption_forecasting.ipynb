{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n\n## Cell 0: Setup Virtual Environment (Local Environment Only)\n\n### What\nWe're creating a dedicated virtual environment for this project to isolate dependencies and ensure reproducibility.\n\n### Why\nUsing a virtual environment is a best practice because:\n- Isolates project dependencies from system Python\n- Prevents version conflicts with other projects\n- Makes the project portable and reproducible\n- Allows specific package versions without affecting other projects\n\n### Technical Details\n**For Local Development**:\n1. Create a virtual environment using Python 3.9+\n2. Activate the virtual environment\n3. Install PyCaret with specific compatible versions\n\n**For Google Colab**: Skip this cell (Colab manages its own environment)\n\n### Instructions\n\n**Option 1: Using venv (recommended)**\n```bash\n# Navigate to your project directory\ncd /Users/banbalagan/Projects/pycaret-automl-examples\n\n# Create virtual environment\npython3.9 -m venv venv\n\n# Activate virtual environment\n# On macOS/Linux:\nsource venv/bin/activate\n# On Windows:\n# venv\\Scripts\\activate\n\n# Verify Python version\npython --version\n\n# Continue to next cell for package installation\n```\n\n**Option 2: Using conda**\n```bash\n# Create conda environment\nconda create -n pycaret-env python=3.9 -y\n\n# Activate environment\nconda activate pycaret-env\n```\n\n### Expected Output\nAfter activation, your terminal prompt should show `(venv)` or `(pycaret-env)` prefix, indicating the virtual environment is active.\n\n### Important Notes\n- Run this in your terminal BEFORE opening Jupyter Notebook\n- After creating/activating the virtual environment, install Jupyter in it:\n  ```bash\n  pip install jupyter notebook\n  ```\n- Then launch Jupyter from within the activated environment:\n  ```bash\n  jupyter notebook\n  ```\n- Select the kernel that corresponds to your virtual environment in Jupyter",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Cell 1: Install and Import Required Libraries\n\n### What\nWe're installing PyCaret with compatible dependencies and importing all necessary Python libraries for our analysis.\n\n### Why\nGoogle Colab comes with pre-installed packages that can conflict with PyCaret's dependencies. For local environments, we install specific package versions to ensure stability and reproducibility.\n\n### Technical Details\n- **Google Colab**: Install compatible versions to avoid runtime crashes\n- **Local Environment**: Install PyCaret with specific versions (Option 2 - recommended)\n- Import all necessary libraries for data analysis and machine learning\n- **Time Series Module**: Additional dependencies for statsmodels and forecasting\n\n### Expected Output\n- **Google Colab**: Installation messages and a reminder to restart the runtime\n- **Local Environment**: Clean installation of all required packages\n\n### IMPORTANT (Google Colab Users)\n\u26a0\ufe0f After running this cell in Colab, you MUST restart the runtime:\n- Click: **Runtime \u2192 Restart runtime** (or Ctrl+M .)\n- After restart, skip this cell and run all other cells normally"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# INSTALLATION CELL - Environment Detection & Package Setup\n# ============================================================\n\nimport sys\nimport os\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"=\" * 60)\n    print(\"\ud83d\udd27 Google Colab Detected\")\n    print(\"=\" * 60)\n    print(\"\ud83d\udce6 Installing PyCaret with compatible dependencies...\")\n    print(\"\u23f3 This will take 2-3 minutes, please be patient...\")\n\n    # Upgrade pip first\n    !pip install -q --upgrade pip\n\n    # Install compatible base packages FIRST (prevents conflicts)\n    print(\"Step 1/4: Installing base packages with compatible versions...\")\n    !pip install -q --upgrade \\\n        numpy>=1.23.0,<2.0.0 \\\n        pandas>=2.0.0,<2.3.0 \\\n        scipy>=1.10.0,<1.14.0 \\\n        scikit-learn>=1.3.0,<1.6.0 \\\n        matplotlib>=3.7.0,<3.9.0\n\n    # Install PyCaret (will use already installed base packages)\n    print(\"Step 2/4: Installing PyCaret...\")\n    !pip install -q pycaret\n\n    # Install time series specific packages\n    print(\"Step 3/4: Installing time series packages...\")\n    !pip install -q statsmodels pmdarima\n\n    # Install additional ML packages\n    print(\"Step 4/4: Installing additional ML packages...\")\n    !pip install -q \\\n        category-encoders \\\n        lightgbm \\\n        xgboost \\\n        catboost \\\n        optuna \\\n        plotly \\\n        kaleido\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"\u2705 Installation Complete!\")\n    print(\"=\" * 60)\n    print(\"\u26a0\ufe0f  CRITICAL: You MUST restart the runtime now!\")\n    print(\"   \ud83d\udc49 Click: Runtime \u2192 Restart runtime (or Ctrl+M .)\")\n    print(\"\ud83d\udd04 After restart:\")\n    print(\"   1. Skip this installation cell\")\n    print(\"   2. Run all other cells normally\")\n    print(\"   3. Everything will work without crashes!\")\n    print(\"=\" * 60)\n\nelse:\n    print(\"=\" * 60)\n    print(\"\ud83d\udccd Local Environment Detected\")\n    print(\"=\" * 60)\n    print(\"Installing PyCaret with specific compatible versions...\")\n    print(\"Using Option 2: Controlled dependency installation\\n\")\n    \n    # Upgrade pip first\n    !pip install -q --upgrade pip\n    \n    # Install base packages with specific versions (Option 2)\n    print(\"Step 1/5: Installing base packages...\")\n    !pip install -q \\\n        numpy>=1.23.0,<2.0.0 \\\n        pandas>=2.0.0,<2.3.0 \\\n        scipy>=1.10.0,<1.14.0 \\\n        scikit-learn>=1.3.0,<1.6.0 \\\n        matplotlib>=3.7.0,<3.9.0 \\\n        seaborn>=0.12.0\n    \n    # Install PyCaret\n    print(\"Step 2/5: Installing PyCaret...\")\n    !pip install -q pycaret\n    \n    # Install time series specific packages\n    print(\"Step 3/5: Installing time series packages...\")\n    !pip install -q statsmodels pmdarima\n    \n    # Install additional ML packages\n    print(\"Step 4/5: Installing additional ML libraries...\")\n    !pip install -q \\\n        category-encoders \\\n        lightgbm \\\n        xgboost \\\n        catboost \\\n        optuna \\\n        plotly \\\n        kaleido\n    \n    # Install notebook support packages\n    print(\"Step 5/5: Installing notebook support packages...\")\n    !pip install -q ipywidgets\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"\u2705 Installation Complete!\")\n    print(\"=\" * 60)\n    print(\"All packages installed successfully in your virtual environment.\")\n    print(\"You can now proceed with running the rest of the notebook.\")\n    print(\"=\" * 60)\n\n# Import libraries after installation\nprint(\"\\n\ud83d\udcda Importing libraries...\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"\\n\u2713 Libraries imported successfully!\")\nprint(f\"   - Python version: {sys.version.split()[0]}\")\nprint(f\"   - Pandas version: {pd.__version__}\")\nprint(f\"   - NumPy version: {np.__version__}\")\nprint(f\"   - Working directory: {os.getcwd()}\")\n\n# Check if running in virtual environment\nif hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):\n    print(f\"   - Virtual environment: Active \u2713\")\n    print(f\"   - Environment path: {sys.prefix}\")\nelse:\n    print(\"   - Virtual environment: Not detected (consider using venv)\")\n    \nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## Cell 2: Load or Create Energy Consumption Data\n\n### What\nLoading or creating energy consumption time series data with realistic patterns.\n\n### Why\nReal energy data has:\n- **Trend**: Increasing over time (growth)\n- **Seasonality**: Daily (peak hours) and yearly (summer/winter)\n- **Noise**: Random fluctuations\n- **Special events**: Holidays, extreme weather\n\n### Technical Details\n- **Local Environment**: Loads from the project's datasets folder if available\n- **Google Colab & Demo**: Creates synthetic time series data with realistic patterns\n- We'll create 2+ years of daily data with upward trend, annual seasonality, weekly patterns, and random noise\n\n### Dataset Locations\n- **Local Path**: `/Users/banbalagan/Projects/pycaret-automl-examples/datasets/time-series/global_energy_consumption.csv`\n- **Demo Mode**: Synthetic data generation for educational purposes\n\n### Expected Output\nTime series dataset with realistic energy patterns."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys\nimport os\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\n# Define dataset paths\nLOCAL_PATH = '/Users/banbalagan/Projects/pycaret-automl-examples/datasets/time-series/global_energy_consumption.csv'\n\n# Try to load from local path first (for local development)\nif not IN_COLAB and os.path.exists(LOCAL_PATH):\n    print(\"=\" * 60)\n    print(\"\ud83d\udcc2 Loading dataset from local path...\")\n    print(\"=\" * 60)\n    print(f\"Path: {LOCAL_PATH}\\n\")\n    df = pd.read_csv(LOCAL_PATH)\n    # Ensure date column is parsed\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'])\n        df.set_index('date', inplace=True)\n    print(f\"\u2713 Dataset loaded successfully from local file!\")\n    use_synthetic = False\n    \nelif not IN_COLAB:\n    # Local environment but file doesn't exist\n    print(\"=\" * 60)\n    print(\"\u26a0\ufe0f  Local path not found...\")\n    print(\"=\" * 60)\n    print(f\"Expected location: {LOCAL_PATH}\")\n    print(\"\\nCreating synthetic energy consumption data for demonstration...\\n\")\n    use_synthetic = True\n    \nelse:\n    # Google Colab - create synthetic data\n    print(\"=\" * 60)\n    print(\"\u2601\ufe0f  Google Colab Detected\")\n    print(\"=\" * 60)\n    print(\"Creating synthetic energy consumption data for demonstration...\\n\")\n    use_synthetic = True\n\n# Create synthetic data if needed\nif use_synthetic or not 'df' in locals():\n    print(\"=\" * 60)\n    print(\"CREATING SYNTHETIC ENERGY CONSUMPTION DATA\")\n    print(\"=\" * 60)\n    \n    # Create date range (2 years of daily data)\n    start_date = '2022-01-01'\n    periods = 730  # 2 years\n    dates = pd.date_range(start=start_date, periods=periods, freq='D')\n    \n    # Components\n    np.random.seed(42)\n    \n    # 1. Trend (gradual increase over time)\n    trend = np.linspace(100, 120, periods)\n    \n    # 2. Annual seasonality (higher in summer/winter)\n    annual_seasonality = 15 * np.sin(2 * np.pi * np.arange(periods) / 365)\n    \n    # 3. Weekly pattern (lower on weekends)\n    weekly_pattern = -5 * (pd.Series(dates).dt.dayofweek >= 5).astype(int).values\n    \n    # 4. Random noise\n    noise = np.random.normal(0, 3, periods)\n    \n    # Combine all components\n    energy_consumption = trend + annual_seasonality + weekly_pattern + noise\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'date': dates,\n        'energy_consumption': energy_consumption\n    })\n    \n    df.set_index('date', inplace=True)\n    \n    print(f\"\\n\u2713 Created {len(df)} days of synthetic energy consumption data\")\n\n# Display basic information\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DATASET INFORMATION\")\nprint(\"=\" * 60)\nprint(f\"Period: {df.index.min().date()} to {df.index.max().date()}\")\nprint(f\"Total observations: {len(df)}\")\nprint(f\"Frequency: Daily\")\n\n# Get the target column name (handle different column names)\ntarget_col = 'energy_consumption' if 'energy_consumption' in df.columns else df.columns[0]\n\nprint(f\"\\nData characteristics:\")\nprint(f\"- Mean consumption: {df[target_col].mean():.2f} MW\")\nprint(f\"- Min: {df[target_col].min():.2f} MW\")\nprint(f\"- Max: {df[target_col].max():.2f} MW\")\nprint(f\"- Std: {df[target_col].std():.2f} MW\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FIRST 10 DAYS\")\nprint(\"=\" * 60)\ndf.head(10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 3: Visualize Time Series\n",
    "\n",
    "### What\n",
    "Plotting the energy consumption over time to identify patterns.\n",
    "\n",
    "### Why\n",
    "Visual inspection reveals:\n",
    "- **Trend**: Is consumption increasing/decreasing?\n",
    "- **Seasonality**: Do patterns repeat?\n",
    "- **Volatility**: How much does it vary?\n",
    "- **Outliers**: Any unusual spikes/drops?\n",
    "\n",
    "### Technical Details\n",
    "Time series visualization is critical for:\n",
    "- Model selection\n",
    "- Feature engineering\n",
    "- Anomaly detection\n",
    "- Business insights\n",
    "\n",
    "### Expected Output\n",
    "Line plot showing energy consumption over 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ENERGY CONSUMPTION TIME SERIES VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Full time series\n",
    "axes[0].plot(df.index, df['energy_consumption'], color='blue', linewidth=1)\n",
    "axes[0].set_title('Energy Consumption Over Time (2 Years)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Date', fontsize=12)\n",
    "axes[0].set_ylabel('Energy Consumption (MW)', fontsize=12)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Zoom into 3 months\n",
    "zoom_df = df['2022-01-01':'2022-03-31']\n",
    "axes[1].plot(zoom_df.index, zoom_df['energy_consumption'], color='green', linewidth=2, marker='o', markersize=3)\n",
    "axes[1].set_title('Energy Consumption (First 3 Months - Detailed View)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Date', fontsize=12)\n",
    "axes[1].set_ylabel('Energy Consumption (MW)', fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Clear upward trend (consumption increasing over time)\")\n",
    "print(\"- Seasonal patterns visible (peaks and troughs)\")\n",
    "print(\"- Weekly cycles (weekday vs weekend)\")\n",
    "print(\"- Random fluctuations around trend\")\n",
    "print(\"\\nThese patterns make this ideal for time series forecasting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 4: Time Series Decomposition\n",
    "\n",
    "### What\n",
    "Decomposing the time series into trend, seasonal, and residual components.\n",
    "\n",
    "### Why\n",
    "Decomposition helps understand:\n",
    "- **Trend**: Long-term direction\n",
    "- **Seasonal**: Repeating patterns\n",
    "- **Residual**: Random noise\n",
    "\n",
    "This guides model selection and feature engineering.\n",
    "\n",
    "### Technical Details\n",
    "**Additive Model**: Y = Trend + Seasonal + Residual\n",
    "**Multiplicative Model**: Y = Trend \u00d7 Seasonal \u00d7 Residual\n",
    "\n",
    "Use additive when seasonal variation is constant, multiplicative when it grows with trend.\n",
    "\n",
    "### Expected Output\n",
    "Four plots showing original, trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TIME SERIES DECOMPOSITION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Perform decomposition\n",
    "decomposition = seasonal_decompose(df['energy_consumption'], model='additive', period=365)\n",
    "\n",
    "# Plot components\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
    "\n",
    "# Original\n",
    "axes[0].plot(df.index, df['energy_consumption'], color='blue', linewidth=1)\n",
    "axes[0].set_ylabel('Original', fontsize=12)\n",
    "axes[0].set_title('Time Series Decomposition', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Trend\n",
    "axes[1].plot(decomposition.trend.index, decomposition.trend, color='red', linewidth=2)\n",
    "axes[1].set_ylabel('Trend', fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Seasonal\n",
    "axes[2].plot(decomposition.seasonal.index, decomposition.seasonal, color='green', linewidth=1)\n",
    "axes[2].set_ylabel('Seasonal', fontsize=12)\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "# Residual\n",
    "axes[3].plot(decomposition.resid.index, decomposition.resid, color='purple', linewidth=0.5)\n",
    "axes[3].set_ylabel('Residual', fontsize=12)\n",
    "axes[3].set_xlabel('Date', fontsize=12)\n",
    "axes[3].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPONENT INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. TREND: Shows long-term increase in energy consumption\")\n",
    "print(\"   - Could be due to: population growth, economic development\")\n",
    "\n",
    "print(\"\\n2. SEASONAL: Regular annual patterns\")\n",
    "print(\"   - Peaks in summer/winter (AC/heating demand)\")\n",
    "print(\"   - Troughs in spring/fall (mild weather)\")\n",
    "\n",
    "print(\"\\n3. RESIDUAL: Random fluctuations after removing trend & seasonality\")\n",
    "print(\"   - Should look like white noise (random)\")\n",
    "print(\"   - Patterns here indicate missed components or outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 5: Train-Test Split for Time Series\n",
    "\n",
    "### What\n",
    "Splitting data temporally (NOT randomly!) into train and test sets.\n",
    "\n",
    "### Why\n",
    "**CRITICAL DIFFERENCE from other ML**:\n",
    "- \u274c **NEVER** randomly shuffle time series data!\n",
    "- \u2713 **ALWAYS** split temporally: past = train, recent = test\n",
    "- We predict future based on past, so test must be chronologically after train\n",
    "\n",
    "### Technical Details\n",
    "Common split:\n",
    "- Train: First 80% of data (older)\n",
    "- Test: Last 20% of data (recent)\n",
    "\n",
    "This simulates real forecasting: using historical data to predict future.\n",
    "\n",
    "### Expected Output\n",
    "Train and test sets with temporal ordering preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate split point (80% train, 20% test)\n",
    "split_point = int(len(df) * 0.8)\n",
    "\n",
    "# Split temporally\n",
    "train = df.iloc[:split_point]\n",
    "test = df.iloc[split_point:]\n",
    "\n",
    "print(f\"\\nTotal data: {len(df)} days\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  - Size: {len(train)} days\")\n",
    "print(f\"  - Period: {train.index.min().date()} to {train.index.max().date()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - Size: {len(test)} days\")\n",
    "print(f\"  - Period: {test.index.min().date()} to {test.index.max().date()}\")\n",
    "\n",
    "# Visualize split\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(train.index, train['energy_consumption'], label='Training Data', color='blue', linewidth=1.5)\n",
    "plt.plot(test.index, test['energy_consumption'], label='Test Data', color='red', linewidth=1.5)\n",
    "plt.axvline(x=train.index[-1], color='black', linestyle='--', linewidth=2, label='Train-Test Split')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Energy Consumption (MW)', fontsize=12)\n",
    "plt.title('Train-Test Split (Temporal)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHY TEMPORAL SPLIT IS CRITICAL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\u2713 Correct: Train on past, test on future\")\n",
    "print(\"  Simulates real-world forecasting scenario\")\n",
    "\n",
    "print(\"\\n\u2717 Wrong: Random shuffle and split\")\n",
    "print(\"  Would leak future information into training!\")\n",
    "print(\"  Unrealistically high performance, fails in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 6: Simple Baseline - Naive Forecast\n",
    "\n",
    "### What\n",
    "Creating a naive baseline forecast: tomorrow = today.\n",
    "\n",
    "### Why\n",
    "**Always start with baselines**!\n",
    "- Naive forecast: Next value = current value\n",
    "- Simple but often surprisingly good\n",
    "- ML models must beat this to be useful\n",
    "\n",
    "### Technical Details\n",
    "Naive forecast assumptions:\n",
    "- No trend\n",
    "- No seasonality\n",
    "- Just persistence\n",
    "\n",
    "Good baseline for stable series.\n",
    "\n",
    "### Expected Output\n",
    "Baseline MAE and RMSE to compare ML models against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NAIVE BASELINE FORECAST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Naive forecast: use last training value for all test predictions\n",
    "naive_forecast = np.full(len(test), train['energy_consumption'].iloc[-1])\n",
    "\n",
    "# Calculate metrics\n",
    "naive_mae = mean_absolute_error(test['energy_consumption'], naive_forecast)\n",
    "naive_rmse = np.sqrt(mean_squared_error(test['energy_consumption'], naive_forecast))\n",
    "naive_mape = np.mean(np.abs((test['energy_consumption'] - naive_forecast) / test['energy_consumption'])) * 100\n",
    "\n",
    "print(\"\\nNaive Forecast Performance:\")\n",
    "print(f\"  MAE:  {naive_mae:.2f} MW\")\n",
    "print(f\"  RMSE: {naive_rmse:.2f} MW\")\n",
    "print(f\"  MAPE: {naive_mape:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(test.index, test['energy_consumption'], label='Actual', color='blue', linewidth=2)\n",
    "plt.plot(test.index, naive_forecast, label='Naive Forecast', color='red', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Energy Consumption (MW)', fontsize=12)\n",
    "plt.title('Naive Baseline Forecast', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BASELINE IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nML models MUST beat this simple baseline to be useful!\")\n",
    "print(\"If your fancy model performs worse than naive forecast,\")\n",
    "print(\"something is wrong with the model or data.\")\n",
    "print(f\"\\nTarget to beat: MAE < {naive_mae:.2f} MW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 7: Statistical Forecasting - ARIMA\n",
    "\n",
    "### What\n",
    "Using ARIMA (AutoRegressive Integrated Moving Average) for forecasting.\n",
    "\n",
    "### Why\n",
    "ARIMA is a classic time series model:\n",
    "- **AR** (AutoRegressive): Uses past values\n",
    "- **I** (Integrated): Handles trends via differencing\n",
    "- **MA** (Moving Average): Uses past errors\n",
    "\n",
    "Great for univariate time series with trends.\n",
    "\n",
    "### Technical Details\n",
    "ARIMA(p, d, q):\n",
    "- p: AR order (lags)\n",
    "- d: Differencing (make stationary)\n",
    "- q: MA order\n",
    "\n",
    "Auto-ARIMA finds best parameters.\n",
    "\n",
    "### Expected Output\n",
    "ARIMA model fitted and forecasts generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ARIMA FORECASTING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Note: For demonstration, we'll use a simpler approach\n",
    "# In production, use statsmodels or pmdarima for full ARIMA\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "print(\"\\nFitting ARIMA model...\")\n",
    "print(\"(This may take a moment)\\n\")\n",
    "\n",
    "# Fit ARIMA model\n",
    "try:\n",
    "    arima_model = ARIMA(train['energy_consumption'], order=(2, 1, 2))\n",
    "    arima_fitted = arima_model.fit()\n",
    "    \n",
    "    # Forecast\n",
    "    arima_forecast = arima_fitted.forecast(steps=len(test))\n",
    "    \n",
    "    # Metrics\n",
    "    arima_mae = mean_absolute_error(test['energy_consumption'], arima_forecast)\n",
    "    arima_rmse = np.sqrt(mean_squared_error(test['energy_consumption'], arima_forecast))\n",
    "    arima_mape = np.mean(np.abs((test['energy_consumption'] - arima_forecast) / test['energy_consumption'])) * 100\n",
    "    \n",
    "    print(\"\u2713 ARIMA model fitted successfully!\")\n",
    "    print(\"\\nARIMA Performance:\")\n",
    "    print(f\"  MAE:  {arima_mae:.2f} MW\")\n",
    "    print(f\"  RMSE: {arima_rmse:.2f} MW\")\n",
    "    print(f\"  MAPE: {arima_mape:.2f}%\")\n",
    "    \n",
    "    # Compare to baseline\n",
    "    improvement = (naive_mae - arima_mae) / naive_mae * 100\n",
    "    print(f\"\\nImprovement over naive: {improvement:.1f}%\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(train.index[-60:], train['energy_consumption'][-60:], label='Training', color='gray', linewidth=1.5)\n",
    "    plt.plot(test.index, test['energy_consumption'], label='Actual', color='blue', linewidth=2)\n",
    "    plt.plot(test.index, arima_forecast, label='ARIMA Forecast', color='green', linewidth=2, linestyle='--')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel('Energy Consumption (MW)', fontsize=12)\n",
    "    plt.title('ARIMA Forecast vs Actual', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ARIMA fitting encountered an issue: {e}\")\n",
    "    print(\"This is common with synthetic data. In production, use real time series.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 8: Machine Learning Approach - Feature Engineering\n",
    "\n",
    "### What\n",
    "Creating features from time series for ML models (regression approach).\n",
    "\n",
    "### Why\n",
    "ML models (Random Forest, XGBoost) need features:\n",
    "- **Lag features**: Previous values (t-1, t-2, ...)\n",
    "- **Rolling statistics**: Moving averages, std\n",
    "- **Time features**: Day of week, month, quarter\n",
    "- **Seasonal indicators**: Holiday, weekend\n",
    "\n",
    "### Technical Details\n",
    "Transform time series into supervised learning:\n",
    "- X: lag features, time features\n",
    "- y: target value\n",
    "\n",
    "### Expected Output\n",
    "Dataset with engineered features ready for ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING FOR ML FORECASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_features(df_input):\n",
    "    \"\"\"\n",
    "    Create time series features for ML models\n",
    "    \"\"\"\n",
    "    df_feat = df_input.copy()\n",
    "    \n",
    "    # Lag features (previous values)\n",
    "    df_feat['lag_1'] = df_feat['energy_consumption'].shift(1)\n",
    "    df_feat['lag_7'] = df_feat['energy_consumption'].shift(7)  # Same day last week\n",
    "    df_feat['lag_30'] = df_feat['energy_consumption'].shift(30)  # Same day last month\n",
    "    \n",
    "    # Rolling statistics\n",
    "    df_feat['rolling_mean_7'] = df_feat['energy_consumption'].rolling(window=7).mean()\n",
    "    df_feat['rolling_std_7'] = df_feat['energy_consumption'].rolling(window=7).std()\n",
    "    \n",
    "    # Time features\n",
    "    df_feat['day_of_week'] = df_feat.index.dayofweek\n",
    "    df_feat['month'] = df_feat.index.month\n",
    "    df_feat['quarter'] = df_feat.index.quarter\n",
    "    df_feat['day_of_year'] = df_feat.index.dayofyear\n",
    "    \n",
    "    # Cyclical encoding for day of week\n",
    "    df_feat['day_sin'] = np.sin(2 * np.pi * df_feat['day_of_week'] / 7)\n",
    "    df_feat['day_cos'] = np.cos(2 * np.pi * df_feat['day_of_week'] / 7)\n",
    "    \n",
    "    # Weekend indicator\n",
    "    df_feat['is_weekend'] = (df_feat.index.dayofweek >= 5).astype(int)\n",
    "    \n",
    "    return df_feat\n",
    "\n",
    "# Create features\n",
    "df_features = create_features(df)\n",
    "\n",
    "# Remove NaN rows (from lag/rolling features)\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "print(\"\\nFeatures created:\")\n",
    "print(df_features.columns.tolist())\n",
    "\n",
    "print(\"\\nSample of featured data:\")\n",
    "display(df_features.head())\n",
    "\n",
    "print(f\"\\nOriginal shape: {df.shape}\")\n",
    "print(f\"After feature engineering: {df_features.shape}\")\n",
    "print(f\"(Rows reduced due to lag/rolling window requirements)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 9: Machine Learning Forecast - Random Forest\n",
    "\n",
    "### What\n",
    "Using Random Forest regressor for time series forecasting.\n",
    "\n",
    "### Why\n",
    "ML models excel at:\n",
    "- Capturing complex non-linear relationships\n",
    "- Handling multiple features\n",
    "- Robust to outliers\n",
    "- No stationarity assumptions\n",
    "\n",
    "### Technical Details\n",
    "Random Forest for time series:\n",
    "- Treats as regression problem\n",
    "- Uses engineered features\n",
    "- Captures interactions automatically\n",
    "\n",
    "### Expected Output\n",
    "ML forecast with improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST TIME SERIES FORECASTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split featured data\n",
    "split_idx = int(len(df_features) * 0.8)\n",
    "train_ml = df_features.iloc[:split_idx]\n",
    "test_ml = df_features.iloc[split_idx:]\n",
    "\n",
    "# Prepare X and y\n",
    "feature_cols = [col for col in df_features.columns if col != 'energy_consumption']\n",
    "X_train = train_ml[feature_cols]\n",
    "y_train = train_ml['energy_consumption']\n",
    "X_test = test_ml[feature_cols]\n",
    "y_test = test_ml['energy_consumption']\n",
    "\n",
    "print(f\"\\nTraining Random Forest on {len(X_train)} samples...\")\n",
    "print(f\"Using {len(feature_cols)} features\\n\")\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "rf_forecast = rf_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "rf_mae = mean_absolute_error(y_test, rf_forecast)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_forecast))\n",
    "rf_mape = np.mean(np.abs((y_test - rf_forecast) / y_test)) * 100\n",
    "\n",
    "print(\"\u2713 Random Forest trained successfully!\")\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(f\"  MAE:  {rf_mae:.2f} MW\")\n",
    "print(f\"  RMSE: {rf_rmse:.2f} MW\")\n",
    "print(f\"  MAPE: {rf_mape:.2f}%\")\n",
    "\n",
    "# Compare to baseline\n",
    "improvement = (naive_mae - rf_mae) / naive_mae * 100\n",
    "print(f\"\\nImprovement over naive baseline: {improvement:.1f}%\")\n",
    "\n",
    "# Feature importance\n",
    "importances = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for idx, row in importances.head(5).iterrows():\n",
    "    print(f\"  {row['feature']:20s}: {row['importance']:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(test_ml.index, y_test, label='Actual', color='blue', linewidth=2)\n",
    "plt.plot(test_ml.index, rf_forecast, label='Random Forest Forecast', color='purple', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Date', fontsize=12)\n",
    "plt.ylabel('Energy Consumption (MW)', fontsize=12)\n",
    "plt.title('Random Forest Forecast vs Actual', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 10: Model Comparison and Final Recommendations\n",
    "\n",
    "### What\n",
    "Comparing all forecasting approaches and providing recommendations.\n",
    "\n",
    "### Why\n",
    "Different models excel in different scenarios:\n",
    "- Simple data: Naive might suffice\n",
    "- Strong trends: ARIMA works well\n",
    "- Complex patterns: ML models shine\n",
    "\n",
    "### Technical Details\n",
    "Model selection depends on:\n",
    "- Data characteristics\n",
    "- Computational resources\n",
    "- Interpretability needs\n",
    "- Production constraints\n",
    "\n",
    "### Expected Output\n",
    "Comparison table and deployment recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Naive Baseline', 'Random Forest'],\n",
    "    'MAE (MW)': [naive_mae, rf_mae],\n",
    "    'RMSE (MW)': [naive_rmse, rf_rmse],\n",
    "    'MAPE (%)': [naive_mape, rf_mape]\n",
    "})\n",
    "\n",
    "# Add ARIMA if available\n",
    "try:\n",
    "    comparison = pd.concat([comparison, pd.DataFrame({\n",
    "        'Model': ['ARIMA'],\n",
    "        'MAE (MW)': [arima_mae],\n",
    "        'RMSE (MW)': [arima_rmse],\n",
    "        'MAPE (%)': [arima_mape]\n",
    "    })], ignore_index=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = comparison['MAE (MW)'].idxmin()\n",
    "best_model = comparison.loc[best_model_idx, 'Model']\n",
    "best_mae = comparison.loc[best_model_idx, 'MAE (MW)']\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Best Model: {best_model}\")\n",
    "print(f\"   MAE: {best_mae:.2f} MW\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS FOR PRODUCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. SHORT-TERM FORECASTING (1-7 days):\")\n",
    "print(\"   - Use ML models (Random Forest, XGBoost)\")\n",
    "print(\"   - Include recent lags and weather data\")\n",
    "print(\"   - Update predictions daily\")\n",
    "\n",
    "print(\"\\n2. LONG-TERM FORECASTING (months-years):\")\n",
    "print(\"   - Use statistical models (ARIMA, ETS) for trends\")\n",
    "print(\"   - Consider seasonal decomposition\")\n",
    "print(\"   - Account for economic indicators\")\n",
    "\n",
    "print(\"\\n3. REAL-TIME OPERATIONS:\")\n",
    "print(\"   - Deploy lightweight models for speed\")\n",
    "print(\"   - Pre-compute features where possible\")\n",
    "print(\"   - Update with streaming data\")\n",
    "\n",
    "print(\"\\n4. MODEL MAINTENANCE:\")\n",
    "print(\"   - Retrain monthly with new data\")\n",
    "print(\"   - Monitor forecast accuracy\")\n",
    "print(\"   - Detect concept drift\")\n",
    "print(\"   - A/B test model updates\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVE BEST MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(rf_model, 'energy_forecast_model.pkl')\n",
    "print(\"\\n\u2713 Model saved as 'energy_forecast_model.pkl'\")\n",
    "print(\"\\nTo use in production:\")\n",
    "print(\"```python\")\n",
    "print(\"import joblib\")\n",
    "print(\"model = joblib.load('energy_forecast_model.pkl')\")\n",
    "print(\"forecast = model.predict(new_features)\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions and Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Time Series Forecasting**: Predicted future energy consumption\n",
    "2. **Multiple Approaches**: Statistical (ARIMA) and ML (Random Forest)\n",
    "3. **Feature Engineering**: Created lag, rolling, and time features\n",
    "4. **Proper Evaluation**: Temporal split, baseline comparison\n",
    "5. **Production Ready**: Model saved for deployment\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "#### Time Series vs Other ML\n",
    "\n",
    "| Aspect | Regular ML | Time Series |\n",
    "|--------|-----------|-------------|\n",
    "| Data Order | Doesn't matter | CRITICAL! |\n",
    "| Split | Random shuffle | Temporal only |\n",
    "| Features | Independent | Autocorrelated |\n",
    "| Validation | K-fold CV | Walk-forward |\n",
    "| Goal | Predict from X | Predict future |\n",
    "\n",
    "#### Time Series Components\n",
    "\n",
    "1. **Trend**: Long-term increase/decrease\n",
    "2. **Seasonality**: Regular repeating patterns\n",
    "3. **Cyclicality**: Longer irregular cycles\n",
    "4. **Noise**: Random fluctuations\n",
    "\n",
    "#### Forecasting Methods\n",
    "\n",
    "**Statistical Models**:\n",
    "- ARIMA: Classic, interpretable\n",
    "- ETS: Exponential smoothing\n",
    "- Prophet: Facebook's robust model\n",
    "- Best for: Clean, well-behaved series\n",
    "\n",
    "**Machine Learning**:\n",
    "- Random Forest, XGBoost, LightGBM\n",
    "- Needs feature engineering\n",
    "- Best for: Complex patterns, multiple features\n",
    "\n",
    "**Deep Learning**:\n",
    "- LSTM, GRU: Sequential models\n",
    "- Transformers: State-of-the-art\n",
    "- Best for: Very complex, long sequences\n",
    "\n",
    "### Business Value\n",
    "\n",
    "**Energy Sector**:\n",
    "- Optimize power generation schedules\n",
    "- Reduce operational costs\n",
    "- Better grid management\n",
    "- Prevent blackouts\n",
    "\n",
    "**Other Applications**:\n",
    "- **Retail**: Demand forecasting\n",
    "- **Finance**: Stock price prediction\n",
    "- **Healthcare**: Patient admission forecasting\n",
    "- **Manufacturing**: Production planning\n",
    "- **Transportation**: Traffic prediction\n",
    "\n",
    "### Critical Concepts\n",
    "\n",
    "**1. Temporal Split**:\n",
    "- NEVER randomly shuffle time series\n",
    "- Always: past = train, future = test\n",
    "\n",
    "**2. Baseline Models**:\n",
    "- Always compare to simple baselines\n",
    "- Naive, seasonal naive, moving average\n",
    "\n",
    "**3. Feature Engineering**:\n",
    "- Lag features crucial for ML\n",
    "- Rolling statistics capture trends\n",
    "- Time features add seasonality\n",
    "\n",
    "**4. Walk-Forward Validation**:\n",
    "- For production: use rolling window\n",
    "- Retrain periodically\n",
    "- Monitor performance drift\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "\u274c **DON'T**:\n",
    "- Shuffle time series data\n",
    "- Use future data in features (data leakage!)\n",
    "- Ignore seasonality\n",
    "- Skip baseline comparison\n",
    "- Forget to check stationarity\n",
    "\n",
    "\u2713 **DO**:\n",
    "- Temporal train-test split\n",
    "- Feature engineering carefully\n",
    "- Decompose series first\n",
    "- Compare multiple models\n",
    "- Monitor forecast accuracy\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "**Pipeline**:\n",
    "1. Data collection (real-time or batch)\n",
    "2. Feature engineering\n",
    "3. Model prediction\n",
    "4. Post-processing (bounds, smoothing)\n",
    "5. Visualization and alerts\n",
    "\n",
    "**Monitoring**:\n",
    "- Track MAE, RMSE over time\n",
    "- Detect forecast degradation\n",
    "- Retrain triggers (schedule or performance)\n",
    "- A/B test new models\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Forecasting: Principles and Practice](https://otexts.com/fpp3/)\n",
    "- [Time Series Analysis in Python](https://www.statsmodels.org/)\n",
    "- [Prophet Forecasting](https://facebook.github.io/prophet/)\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Bala Anbalagan  \n",
    "**Date**: October 2025  \n",
    "**Data**: Synthetic energy consumption  \n",
    "**License**: MIT  \n",
    "\n",
    "---\n",
    "\n",
    "## Thank you for completing all 6 PyCaret tutorials!\n",
    "\n",
    "**Key Achievement**: Mastered time series forecasting - the most complex ML task!\n",
    "\n",
    "**Main Insight**: Time series requires special treatment due to temporal dependencies. Never shuffle, always split temporally!\n",
    "\n",
    "**Next Steps**:\n",
    "- Apply to real energy/sales/stock data\n",
    "- Try deep learning (LSTM, Transformer)\n",
    "- Deploy for production forecasting\n",
    "\n",
    "**Congratulations on completing all 6 PyCaret AutoML tutorials! \ud83c\udf89**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}