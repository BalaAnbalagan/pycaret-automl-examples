{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n\n## Cell 0: Setup Virtual Environment (Local Environment Only)\n\n### What\nWe're creating a dedicated virtual environment for this project to isolate dependencies and ensure reproducibility.\n\n### Why\nUsing a virtual environment is a best practice because:\n- Isolates project dependencies from system Python\n- Prevents version conflicts with other projects\n- Makes the project portable and reproducible\n- Allows specific package versions without affecting other projects\n\n### Technical Details\n**For Local Development**:\n1. Create a virtual environment using Python 3.9+\n2. Activate the virtual environment\n3. Install PyCaret with specific compatible versions\n\n**For Google Colab**: Skip this cell (Colab manages its own environment)\n\n### Instructions\n\n**Option 1: Using venv (recommended)**\n```bash\n# Navigate to your project directory\ncd /Users/banbalagan/Projects/pycaret-automl-examples\n\n# Create virtual environment\npython3.9 -m venv venv\n\n# Activate virtual environment\n# On macOS/Linux:\nsource venv/bin/activate\n# On Windows:\n# venv\\Scripts\\activate\n\n# Verify Python version\npython --version\n\n# Continue to next cell for package installation\n```\n\n**Option 2: Using conda**\n```bash\n# Create conda environment\nconda create -n pycaret-env python=3.9 -y\n\n# Activate environment\nconda activate pycaret-env\n```\n\n### Expected Output\nAfter activation, your terminal prompt should show `(venv)` or `(pycaret-env)` prefix, indicating the virtual environment is active.\n\n### Important Notes\n- Run this in your terminal BEFORE opening Jupyter Notebook\n- After creating/activating the virtual environment, install Jupyter in it:\n  ```bash\n  pip install jupyter notebook\n  ```\n- Then launch Jupyter from within the activated environment:\n  ```bash\n  jupyter notebook\n  ```\n- Select the kernel that corresponds to your virtual environment in Jupyter",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Cell 1: Install and Import Required Libraries\n\n### What\nWe're installing PyCaret with compatible dependencies and importing all necessary Python libraries for our analysis.\n\n### Why\nGoogle Colab comes with pre-installed packages that can conflict with PyCaret's dependencies. For local environments, we install specific package versions to ensure stability and reproducibility.\n\n### Technical Details\n- **Google Colab**: Install compatible versions to avoid runtime crashes\n- **Local Environment**: Install PyCaret with specific versions (Option 2 - recommended)\n- Import all necessary libraries for data analysis and machine learning\n\n### Expected Output\n- **Google Colab**: Installation messages and a reminder to restart the runtime\n- **Local Environment**: Clean installation of all required packages\n\n### IMPORTANT (Google Colab Users)\n‚ö†Ô∏è After running this cell in Colab, you MUST restart the runtime:\n- Click: **Runtime ‚Üí Restart runtime** (or Ctrl+M .)\n- After restart, skip this cell and run all other cells normally"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# INSTALLATION CELL - Environment Detection & Package Setup\n# ============================================================\n\nimport sys\nimport os\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"=\" * 60)\n    print(\"üîß Google Colab Detected\")\n    print(\"=\" * 60)\n    print(\"üì¶ Installing PyCaret with compatible dependencies...\")\n    print(\"‚è≥ This will take 2-3 minutes, please be patient...\")\n\n    # Upgrade pip first\n    !pip install -q --upgrade pip\n\n    # Install compatible base packages FIRST (prevents conflicts)\n    print(\"Step 1/3: Installing base packages with compatible versions...\")\n    !pip install -q --upgrade \\\n        numpy>=1.23.0,<2.0.0 \\\n        pandas>=2.0.0,<2.3.0 \\\n        scipy>=1.10.0,<1.14.0 \\\n        scikit-learn>=1.3.0,<1.6.0 \\\n        matplotlib>=3.7.0,<3.9.0\n\n    # Install PyCaret (will use already installed base packages)\n    print(\"Step 2/3: Installing PyCaret...\")\n    !pip install -q pycaret\n\n    # Install additional ML packages\n    print(\"Step 3/3: Installing additional ML packages...\")\n    !pip install -q \\\n        category-encoders \\\n        lightgbm \\\n        xgboost \\\n        catboost \\\n        optuna \\\n        plotly \\\n        kaleido\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚úÖ Installation Complete!\")\n    print(\"=\" * 60)\n    print(\"‚ö†Ô∏è  CRITICAL: You MUST restart the runtime now!\")\n    print(\"   üëâ Click: Runtime ‚Üí Restart runtime (or Ctrl+M .)\")\n    print(\"üîÑ After restart:\")\n    print(\"   1. Skip this installation cell\")\n    print(\"   2. Run all other cells normally\")\n    print(\"   3. Everything will work without crashes!\")\n    print(\"=\" * 60)\n\nelse:\n    print(\"=\" * 60)\n    print(\"üìç Local Environment Detected\")\n    print(\"=\" * 60)\n    print(\"Installing PyCaret with specific compatible versions...\")\n    print(\"Using Option 2: Controlled dependency installation\\n\")\n    \n    # Upgrade pip first\n    !pip install -q --upgrade pip\n    \n    # Install base packages with specific versions (Option 2)\n    print(\"Step 1/4: Installing base packages...\")\n    !pip install -q \\\n        numpy>=1.23.0,<2.0.0 \\\n        pandas>=2.0.0,<2.3.0 \\\n        scipy>=1.10.0,<1.14.0 \\\n        scikit-learn>=1.3.0,<1.6.0 \\\n        matplotlib>=3.7.0,<3.9.0 \\\n        seaborn>=0.12.0\n    \n    # Install PyCaret\n    print(\"Step 2/4: Installing PyCaret...\")\n    !pip install -q pycaret\n    \n    # Install additional ML packages\n    print(\"Step 3/4: Installing additional ML libraries...\")\n    !pip install -q \\\n        category-encoders \\\n        lightgbm \\\n        xgboost \\\n        catboost \\\n        optuna \\\n        plotly \\\n        kaleido\n    \n    # Install notebook support packages\n    print(\"Step 4/4: Installing notebook support packages...\")\n    !pip install -q ipywidgets\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"‚úÖ Installation Complete!\")\n    print(\"=\" * 60)\n    print(\"All packages installed successfully in your virtual environment.\")\n    print(\"You can now proceed with running the rest of the notebook.\")\n    print(\"=\" * 60)\n\n# Import libraries after installation\nprint(\"\\nüìö Importing libraries...\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"\\n‚úì Libraries imported successfully!\")\nprint(f\"   - Python version: {sys.version.split()[0]}\")\nprint(f\"   - Pandas version: {pd.__version__}\")\nprint(f\"   - NumPy version: {np.__version__}\")\nprint(f\"   - Working directory: {os.getcwd()}\")\n\n# Check if running in virtual environment\nif hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):\n    print(f\"   - Virtual environment: Active ‚úì\")\n    print(f\"   - Environment path: {sys.prefix}\")\nelse:\n    print(\"   - Virtual environment: Not detected (consider using venv)\")\n    \nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## Cell 2: Load Wholesale Customers Dataset\n\n### What\nLoading the wholesale customers dataset with annual spending patterns.\n\n### Why\nThis dataset is perfect for clustering:\n- **Continuous features**: Spending amounts\n- **Natural groupings**: Different customer types exist\n- **Business relevance**: Real-world segmentation problem\n- **No labels**: Truly unsupervised\n\n### Technical Details\n- **Local Environment**: Loads from the project's datasets folder\n- **Google Colab**: Loads from a public URL\n- Features represent annual spending in monetary units across 6 product categories\n\n### Dataset Locations\n- **Local Path**: `/Users/banbalagan/Projects/pycaret-automl-examples/datasets/clustering/Wholesale customers data.csv`\n- **Remote URL**: For Colab users or if local file is not available\n\n### Expected Output\nDataset with 440 customers and 8 columns."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys\nimport os\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\n# Define dataset paths\nLOCAL_PATH = '/Users/banbalagan/Projects/pycaret-automl-examples/datasets/clustering/Wholesale customers data.csv'\nREMOTE_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv'\n\n# Try to load from local path first (for local development)\nif not IN_COLAB and os.path.exists(LOCAL_PATH):\n    print(\"=\" * 60)\n    print(\"üìÇ Loading dataset from local path...\")\n    print(\"=\" * 60)\n    print(f\"Path: {LOCAL_PATH}\\n\")\n    df = pd.read_csv(LOCAL_PATH)\n    print(f\"‚úì Dataset loaded successfully from local file!\")\n    \nelif not IN_COLAB:\n    # Local environment but file doesn't exist - check relative path\n    print(\"=\" * 60)\n    print(\"‚ö†Ô∏è  Local path not found, trying relative path...\")\n    print(\"=\" * 60)\n    \n    # Try relative path from notebook location\n    relative_paths = [\n        '../datasets/clustering/Wholesale customers data.csv',\n        '../../datasets/clustering/Wholesale customers data.csv',\n        'Wholesale customers data.csv'\n    ]\n    \n    dataset_loaded = False\n    for rel_path in relative_paths:\n        if os.path.exists(rel_path):\n            print(f\"‚úì Found dataset at: {rel_path}\\n\")\n            df = pd.read_csv(rel_path)\n            dataset_loaded = True\n            print(f\"‚úì Dataset loaded successfully from relative path!\")\n            break\n    \n    if not dataset_loaded:\n        print(\"‚ö†Ô∏è  Could not find local dataset file.\")\n        print(f\"Expected location: {LOCAL_PATH}\")\n        print(\"\\nFalling back to remote URL...\\n\")\n        df = pd.read_csv(REMOTE_URL)\n        print(f\"‚úì Dataset loaded successfully from remote URL!\")\n        \nelse:\n    # Google Colab - always use remote URL\n    print(\"=\" * 60)\n    print(\"‚òÅÔ∏è  Loading dataset from remote URL (Google Colab)...\")\n    print(\"=\" * 60)\n    print(f\"URL: {REMOTE_URL}\\n\")\n    df = pd.read_csv(REMOTE_URL)\n    print(f\"‚úì Dataset loaded successfully from remote URL!\")\n\n# Display basic information\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DATASET INFORMATION\")\nprint(\"=\" * 60)\nprint(f\"Shape: {df.shape[0]} customers, {df.shape[1]} features\")\nprint(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COLUMN NAMES\")\nprint(\"=\" * 60)\nprint(f\"Features: {list(df.columns)}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FIRST 5 ROWS\")\nprint(\"=\" * 60)\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nShape: {df.shape[0]} customers, {df.shape[1]} features\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 3: Data Exploration\n",
    "\n",
    "### What\n",
    "Exploring the structure and statistics of customer spending data.\n",
    "\n",
    "### Why\n",
    "Understanding the data helps:\n",
    "- Identify spending patterns\n",
    "- Check for outliers (very important in clustering!)\n",
    "- Understand feature scales\n",
    "- Determine if normalization needed\n",
    "\n",
    "### Technical Details\n",
    "Spending features have very different scales - some in thousands, others in hundreds. Normalization will be critical!\n",
    "\n",
    "### Expected Output\n",
    "Summary statistics showing wide range of spending patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY OBSERVATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"- Features have very different scales (100s to 10,000s)\")\n",
    "print(\"- Large standard deviations indicate diverse customer base\")\n",
    "print(\"- Some customers spend heavily in certain categories\")\n",
    "print(\"- Normalization will be essential for clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 4: Spending Distribution Analysis\n",
    "\n",
    "### What\n",
    "Visualizing spending distributions across the 6 product categories.\n",
    "\n",
    "### Why\n",
    "Understanding distributions helps:\n",
    "- Identify high-spending vs low-spending customers\n",
    "- See which categories vary most\n",
    "- Spot outliers (customers with extreme spending)\n",
    "- Guide clustering approach\n",
    "\n",
    "### Technical Details\n",
    "We'll create box plots for each spending category to see:\n",
    "- Median spending\n",
    "- Spread (IQR)\n",
    "- Outliers\n",
    "\n",
    "### Expected Output\n",
    "Box plots showing spending distribution for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SPENDING DISTRIBUTION ACROSS CATEGORIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select spending columns\n",
    "spending_cols = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(spending_cols):\n",
    "    axes[idx].boxplot(df[col])\n",
    "    axes[idx].set_title(f'{col} Spending', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Monetary Units', fontsize=10)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Fresh and Grocery show highest spending and variation\")\n",
    "print(\"- Many outliers (high-spending customers) in each category\")\n",
    "print(\"- Different customers focus on different product categories\")\n",
    "print(\"- This diversity suggests natural customer segments exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 5: Correlation Analysis\n",
    "\n",
    "### What\n",
    "Analyzing correlations between different spending categories.\n",
    "\n",
    "### Why\n",
    "Correlation reveals:\n",
    "- Which products are bought together\n",
    "- Customer purchasing patterns\n",
    "- Potential customer types (e.g., grocery-focused vs fresh-focused)\n",
    "\n",
    "### Technical Details\n",
    "High correlation between categories suggests customers who buy one product category also buy another.\n",
    "\n",
    "### Expected Output\n",
    "Heatmap showing correlations between spending categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION BETWEEN SPENDING CATEGORIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate correlation\n",
    "corr_matrix = df[spending_cols].corr()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Spending Category Correlations', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- High positive correlation: Products bought together\")\n",
    "print(\"- Example: Grocery, Milk, Detergents_Paper often correlated\")\n",
    "print(\"  (suggests retail customers buying household essentials)\")\n",
    "print(\"- Fresh spending sometimes independent\")\n",
    "print(\"  (suggests restaurant/cafe customers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 6: PyCaret Setup for Clustering\n",
    "\n",
    "### What\n",
    "Initializing PyCaret's clustering environment for unsupervised learning.\n",
    "\n",
    "### Why\n",
    "Clustering setup is unique:\n",
    "- **No target variable** (unsupervised!)\n",
    "- **Normalization critical**: Features must be on same scale\n",
    "- **Different preprocessing**: No train/test split (use all data)\n",
    "\n",
    "### Technical Details\n",
    "PyCaret will:\n",
    "- Normalize all features (essential for distance-based clustering)\n",
    "- Handle any transformations\n",
    "- Prepare data for multiple clustering algorithms\n",
    "\n",
    "**Key Difference from Supervised Learning**:\n",
    "- No train/test split\n",
    "- No cross-validation\n",
    "- Evaluation uses internal metrics (silhouette, Davies-Bouldin, etc.)\n",
    "\n",
    "### Expected Output\n",
    "Setup summary confirming clustering configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pycaret.clustering import *\n\nprint(\"=\" * 60)\nprint(\"PYCARET SETUP - CLUSTERING (UNSUPERVISED)\")\nprint(\"=\" * 60)\nprint(\"\\nConfiguring unsupervised learning environment...\\n\")\n\n# Initialize clustering setup\n# Changed from session_seed to session_id for PyCaret 3.x\ncluster_setup = setup(\n    data=df,\n    normalize=True,  # CRITICAL for clustering!\n    session_id=42,\n    verbose=True\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úì Clustering setup completed!\")\nprint(\"=\" * 60)\nprint(\"\\nKey Differences from Supervised Learning:\")\nprint(\"- NO target variable (unsupervised)\")\nprint(\"- NO train/test split (use all 440 customers)\")\nprint(\"- Evaluation uses internal metrics (silhouette, etc.)\")\nprint(\"\\nReady to discover customer segments!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 7: Create and Compare Different Clustering Models\n",
    "\n",
    "### What\n",
    "Creating multiple clustering models with different algorithms and cluster numbers.\n",
    "\n",
    "### Why\n",
    "Different clustering algorithms have different strengths:\n",
    "- **KMeans**: Fast, works well with spherical clusters\n",
    "- **Hierarchical**: No need to specify K upfront, creates dendrogram\n",
    "- **DBSCAN**: Finds arbitrary shapes, identifies outliers\n",
    "- **Gaussian Mixture**: Probabilistic clustering\n",
    "\n",
    "### Technical Details\n",
    "We need to determine:\n",
    "1. **Which algorithm** works best\n",
    "2. **How many clusters** (K) are optimal\n",
    "\n",
    "PyCaret provides metrics to compare:\n",
    "- **Silhouette Score**: How well separated clusters are (0-1, higher better)\n",
    "- **Calinski-Harabasz**: Ratio of between/within cluster variance (higher better)\n",
    "- **Davies-Bouldin**: Average similarity between clusters (lower better)\n",
    "\n",
    "### Expected Output\n",
    "Table comparing different clustering models and their quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPARING CLUSTERING ALGORITHMS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTesting different algorithms to find best customer segmentation...\\n\")\n",
    "\n",
    "# Create KMeans with different K values\n",
    "print(\"Testing KMeans with K=3, 4, 5...\")\n",
    "kmeans_3 = create_model('kmeans', num_clusters=3)\n",
    "kmeans_4 = create_model('kmeans', num_clusters=4)\n",
    "kmeans_5 = create_model('kmeans', num_clusters=5)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Clustering Quality Metrics Explained:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n- Silhouette Score (0-1): How well separated clusters are\")\n",
    "print(\"  Higher is better. >0.5 is good, >0.7 is excellent\")\n",
    "print(\"\\n- Calinski-Harabasz: Between vs within cluster variance\")\n",
    "print(\"  Higher is better. Indicates dense, well-separated clusters\")\n",
    "print(\"\\n- Davies-Bouldin: Average similarity between clusters\")\n",
    "print(\"  Lower is better. Measures cluster separation\")\n",
    "\n",
    "print(\"\\nNote: We'll use Silhouette and Elbow method to choose optimal K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 8: Elbow Method for Optimal K\n",
    "\n",
    "### What\n",
    "Using the Elbow Method to determine the optimal number of customer segments.\n",
    "\n",
    "### Why\n",
    "The Elbow Method helps find optimal K:\n",
    "- Plots within-cluster sum of squares (WCSS) vs K\n",
    "- Look for the \"elbow\" - point where adding clusters provides diminishing returns\n",
    "- Balance between simplicity (few clusters) and detail (many clusters)\n",
    "\n",
    "### Technical Details\n",
    "**WCSS** (Within-Cluster Sum of Squares):\n",
    "- Measures how compact clusters are\n",
    "- Always decreases as K increases\n",
    "- Elbow = point where decrease slows significantly\n",
    "\n",
    "### Expected Output\n",
    "Elbow plot showing optimal number of clusters (typically 3-5 for this dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ELBOW METHOD - FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyCaret's elbow plot\n",
    "plot_model(kmeans_4, plot='elbow')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HOW TO READ THE ELBOW PLOT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Look for the 'elbow' - point where line starts to flatten\")\n",
    "print(\"2. This is where adding more clusters gives diminishing returns\")\n",
    "print(\"3. Balance model complexity with interpretability\")\n",
    "print(\"\\nFor business segmentation: 3-5 clusters typically optimal\")\n",
    "print(\"- Too few: Lose important distinctions\")\n",
    "print(\"- Too many: Hard to create targeted strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 9: Silhouette Analysis\n",
    "\n",
    "### What\n",
    "Analyzing the Silhouette Score to validate cluster quality.\n",
    "\n",
    "### Why\n",
    "Silhouette analysis shows:\n",
    "- How well each customer fits their cluster\n",
    "- Whether clusters are well-separated\n",
    "- If any customers are in the wrong cluster\n",
    "\n",
    "### Technical Details\n",
    "**Silhouette Score** for each point measures:\n",
    "- Distance to own cluster vs distance to nearest other cluster\n",
    "- Score close to +1: Well clustered\n",
    "- Score close to 0: On cluster boundary\n",
    "- Score close to -1: Probably in wrong cluster\n",
    "\n",
    "### Expected Output\n",
    "Silhouette plot showing cluster quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SILHOUETTE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAnalyzing cluster separation quality...\\n\")\n",
    "\n",
    "# Silhouette plot\n",
    "plot_model(kmeans_4, plot='silhouette')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETING SILHOUETTE PLOT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n- Each horizontal bar represents one cluster\")\n",
    "print(\"- Width shows how many customers in that cluster\")\n",
    "print(\"- Thickness at different x-values shows silhouette scores\")\n",
    "print(\"\\nIdeal characteristics:\")\n",
    "print(\"- All bars extend well past the average line (red dashed)\")\n",
    "print(\"- Bars have similar thickness (balanced cluster sizes)\")\n",
    "print(\"- Few or no bars below 0 (no misclassified customers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 10: Assign Customers to Clusters\n",
    "\n",
    "### What\n",
    "Assigning each customer to their optimal cluster and adding cluster labels to our dataset.\n",
    "\n",
    "### Why\n",
    "Once we have good clusters, we need to:\n",
    "- Assign each customer to a cluster\n",
    "- Analyze what makes each cluster unique\n",
    "- Create actionable business segments\n",
    "\n",
    "### Technical Details\n",
    "`assign_model()` adds a 'Cluster' column showing which segment each customer belongs to.\n",
    "\n",
    "### Expected Output\n",
    "Original dataset with added 'Cluster' column showing segment membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ASSIGNING CUSTOMERS TO SEGMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Assign clusters (using 4 clusters based on elbow method)\n",
    "clustered_df = assign_model(kmeans_4)\n",
    "\n",
    "print(f\"\\n‚úì All {len(clustered_df)} customers assigned to clusters!\")\n",
    "print(\"\\nCustomers per Cluster:\")\n",
    "print(clustered_df['Cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSample of clustered data:\")\n",
    "display(clustered_df[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Cluster']].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Next: Profile each cluster to understand customer segments!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 11: Cluster Profiling - Understanding Each Segment\n",
    "\n",
    "### What\n",
    "Analyzing the characteristics of each customer segment by examining average spending patterns.\n",
    "\n",
    "### Why\n",
    "This is where clustering becomes actionable:\n",
    "- **What defines each segment?** High grocery vs high fresh spending?\n",
    "- **Business naming**: \"Restaurant Customers\", \"Retail Stores\", etc.\n",
    "- **Targeted strategies**: Different marketing for each segment\n",
    "\n",
    "### Technical Details\n",
    "We'll calculate mean spending per cluster across all 6 product categories.\n",
    "\n",
    "### Expected Output\n",
    "Table and visualizations showing average spending profile for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CLUSTER PROFILING - UNDERSTANDING CUSTOMER SEGMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate mean spending per cluster\n",
    "cluster_profiles = clustered_df.groupby('Cluster')[spending_cols].mean()\n",
    "\n",
    "print(\"\\nAverage Spending by Cluster:\")\n",
    "display(cluster_profiles.round(0))\n",
    "\n",
    "# Visualization\n",
    "cluster_profiles_T = cluster_profiles.T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "cluster_profiles_T.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Average Spending by Customer Segment', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Product Category', fontsize=12)\n",
    "ax.set_ylabel('Average Spending (Monetary Units)', fontsize=12)\n",
    "ax.legend(title='Cluster', title_fontsize=12, fontsize=10)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SEGMENT INTERPRETATION (Example)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify cluster characteristics\n",
    "for cluster_id in sorted(clustered_df['Cluster'].unique()):\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    top_category = profile.idxmax()\n",
    "    top_spending = profile.max()\n",
    "    size = len(clustered_df[clustered_df['Cluster'] == cluster_id])\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}: ({size} customers)\")\n",
    "    print(f\"  Highest spending: {top_category} (${top_spending:,.0f})\")\n",
    "    print(f\"  Profile: \", end=\"\")\n",
    "    \n",
    "    # Simple characterization\n",
    "    if top_category in ['Fresh', 'Frozen']:\n",
    "        print(\"Likely RESTAURANTS/CAFES (Fresh food focus)\")\n",
    "    elif top_category in ['Grocery', 'Milk', 'Detergents_Paper']:\n",
    "        print(\"Likely RETAIL STORES (Household goods focus)\")\n",
    "    else:\n",
    "        print(\"SPECIALIZED customer type\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Use these insights to tailor marketing and inventory!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 12: Cluster Visualization with PCA\n",
    "\n",
    "### What\n",
    "Visualizing clusters in 2D space using Principal Component Analysis (PCA).\n",
    "\n",
    "### Why\n",
    "We have 6 spending dimensions - impossible to visualize directly!\n",
    "PCA reduces to 2D while preserving cluster structure:\n",
    "- See if clusters are well-separated\n",
    "- Identify overlapping segments\n",
    "- Visualize cluster shapes\n",
    "\n",
    "### Technical Details\n",
    "PCA projects 6D data onto 2D plane:\n",
    "- PC1 and PC2 capture maximum variance\n",
    "- Clusters that separate in 6D should separate in 2D\n",
    "\n",
    "### Expected Output\n",
    "2D scatter plot showing clusters in reduced dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"2D CLUSTER VISUALIZATION (PCA)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nReducing 6 dimensions to 2 for visualization...\\n\")\n",
    "\n",
    "# PyCaret's cluster visualization\n",
    "plot_model(kmeans_4, plot='cluster')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETING THE PLOT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n- Each point represents one customer\")\n",
    "print(\"- Colors show cluster membership\")\n",
    "print(\"- PCA reduces 6D spending data to 2D\")\n",
    "print(\"- PC1 & PC2: Principal components capturing most variance\")\n",
    "print(\"\\nGood clustering shows:\")\n",
    "print(\"- Clear color separation (distinct segments)\")\n",
    "print(\"- Minimal overlap between clusters\")\n",
    "print(\"- Compact clusters with space between them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 13: Distribution of Clusters\n",
    "\n",
    "### What\n",
    "Analyzing the size and distribution of discovered customer segments.\n",
    "\n",
    "### Why\n",
    "Understanding segment sizes helps:\n",
    "- Prioritize marketing efforts\n",
    "- Allocate resources\n",
    "- Assess market opportunity\n",
    "\n",
    "### Technical Details\n",
    "Check for:\n",
    "- Balanced clusters (similar sizes)\n",
    "- Or dominant segments with small niches\n",
    "\n",
    "### Expected Output\n",
    "Visualization showing relative size of each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CUSTOMER SEGMENT DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Plot distribution\n",
    "plot_model(kmeans_4, plot='distribution')\n",
    "\n",
    "# Summary statistics\n",
    "cluster_counts = clustered_df['Cluster'].value_counts().sort_index()\n",
    "cluster_pcts = (cluster_counts / len(clustered_df) * 100).round(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SEGMENT SIZES\")\n",
    "print(\"=\" * 60)\n",
    "for cluster_id in sorted(clustered_df['Cluster'].unique()):\n",
    "    count = cluster_counts[cluster_id]\n",
    "    pct = cluster_pcts[cluster_id]\n",
    "    print(f\"\\nCluster {cluster_id}: {count} customers ({pct}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if cluster_counts.std() < cluster_counts.mean() * 0.3:\n",
    "    print(\"‚úì Balanced segments - good for broad strategies\")\n",
    "else:\n",
    "    print(\"‚ö† Unbalanced segments - focus on largest segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 14: Business Recommendations per Segment\n",
    "\n",
    "### What\n",
    "Creating actionable business recommendations for each customer segment.\n",
    "\n",
    "### Why\n",
    "The ultimate goal of clustering:\n",
    "- Translate patterns into actions\n",
    "- Segment-specific strategies\n",
    "- Measurable business outcomes\n",
    "\n",
    "### Technical Details\n",
    "Based on spending profiles, we'll suggest:\n",
    "- Marketing approaches\n",
    "- Product focus\n",
    "- Service levels\n",
    "- Pricing strategies\n",
    "\n",
    "### Expected Output\n",
    "Business recommendations for each discovered segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BUSINESS RECOMMENDATIONS BY SEGMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cluster_id in sorted(clustered_df['Cluster'].unique()):\n",
    "    profile = cluster_profiles.loc[cluster_id]\n",
    "    size = len(clustered_df[clustered_df['Cluster'] == cluster_id])\n",
    "    top_3_categories = profile.nlargest(3)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id}: {size} Customers ({size/len(clustered_df)*100:.1f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nTop 3 Spending Categories:\")\n",
    "    for cat, amount in top_3_categories.items():\n",
    "        print(f\"  {cat}: ${amount:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nRecommended Actions:\")\n",
    "    \n",
    "    # Generate recommendations based on profile\n",
    "    if profile['Fresh'] > profile.mean() * 1.5:\n",
    "        print(\"  üìç SEGMENT TYPE: Fresh Food Focused (Restaurants/Cafes)\")\n",
    "        print(\"  ‚úì Marketing: Emphasize fresh product quality and variety\")\n",
    "        print(\"  ‚úì Inventory: Ensure fresh product availability and fast turnover\")\n",
    "        print(\"  ‚úì Service: Priority delivery for perishables\")\n",
    "    elif profile[['Grocery', 'Milk', 'Detergents_Paper']].mean() > profile.mean():\n",
    "        print(\"  üìç SEGMENT TYPE: Retail/Household Goods Focused\")\n",
    "        print(\"  ‚úì Marketing: Bulk discounts and loyalty programs\")\n",
    "        print(\"  ‚úì Inventory: Stock household essentials in volume\")\n",
    "        print(\"  ‚úì Service: Flexible delivery schedules\")\n",
    "    elif profile['Frozen'] > profile.mean() * 1.2:\n",
    "        print(\"  üìç SEGMENT TYPE: Frozen Products Specialist\")\n",
    "        print(\"  ‚úì Marketing: Highlight frozen product range and storage\")\n",
    "        print(\"  ‚úì Inventory: Expand frozen category offerings\")\n",
    "        print(\"  ‚úì Service: Ensure cold chain integrity\")\n",
    "    else:\n",
    "        print(\"  üìç SEGMENT TYPE: Diversified/Balanced Customers\")\n",
    "        print(\"  ‚úì Marketing: Cross-category promotions\")\n",
    "        print(\"  ‚úì Inventory: Maintain balanced stock levels\")\n",
    "        print(\"  ‚úì Service: Standard delivery options\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMPLEMENTATION STRATEGY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Assign sales team members to specific segments\")\n",
    "print(\"2. Create segment-specific marketing materials\")\n",
    "print(\"3. Adjust inventory based on segment demand\")\n",
    "print(\"4. Track segment profitability and satisfaction\")\n",
    "print(\"5. Refine segments quarterly with new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 15: Save Clustering Model\n",
    "\n",
    "### What\n",
    "Saving the trained clustering model for future use.\n",
    "\n",
    "### Why\n",
    "The model can be used to:\n",
    "- Assign new customers to existing segments\n",
    "- Monitor segment drift over time\n",
    "- Integrate into CRM systems\n",
    "- Automate segmentation\n",
    "\n",
    "### Technical Details\n",
    "Saved model includes:\n",
    "- Trained clustering algorithm\n",
    "- Preprocessing steps (normalization)\n",
    "- Cluster centers\n",
    "\n",
    "### Expected Output\n",
    "Model file saved and ready for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAVING CUSTOMER SEGMENTATION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the model\n",
    "model_name = 'customer_segmentation_model'\n",
    "save_model(kmeans_4, model_name)\n",
    "\n",
    "print(f\"\\n‚úì Model saved as '{model_name}.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEPLOYMENT APPLICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Assign new customers to segments automatically\")\n",
    "print(\"2. Integrate with CRM for personalized service\")\n",
    "print(\"3. Power targeted marketing campaigns\")\n",
    "print(\"4. Optimize inventory by segment\")\n",
    "print(\"5. Track segment evolution over time\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TO USE THE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n```python\")\n",
    "print(\"from pycaret.clustering import load_model, predict_model\")\n",
    "print(f\"model = load_model('{model_name}')\")\n",
    "print(\"segment = predict_model(model, data=new_customer)\")\n",
    "print(\"```\")\n",
    "\n",
    "# Export segmented customers\n",
    "clustered_df.to_csv('segmented_customers.csv', index=False)\n",
    "print(\"\\n‚úì Segmented customer data exported to 'segmented_customers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions and Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Unsupervised Learning**: Discovered natural customer segments without labeled data\n",
    "2. **Customer Segmentation**: Grouped 440 customers into meaningful segments\n",
    "3. **Cluster Profiling**: Identified unique characteristics of each segment\n",
    "4. **Business Insights**: Created actionable recommendations per segment\n",
    "5. **Model Deployment**: Saved model for ongoing segmentation\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "#### Unsupervised vs Supervised Learning\n",
    "\n",
    "| Aspect | Supervised | Unsupervised (Clustering) |\n",
    "|--------|-----------|---------------------------|\n",
    "| Labels | Have target variable | No labels - discover patterns |\n",
    "| Goal | Predict outcomes | Find natural groupings |\n",
    "| Evaluation | Accuracy, RMSE | Silhouette, Davies-Bouldin |\n",
    "| Train/Test | Split data | Use all data |\n",
    "| Example | \"Predict cost\" | \"Find customer types\" |\n",
    "\n",
    "#### Technical Skills\n",
    "- **Clustering Algorithms**: KMeans, Hierarchical, DBSCAN\n",
    "- **Optimal K Selection**: Elbow method, Silhouette analysis\n",
    "- **Cluster Evaluation**: Internal metrics (no ground truth)\n",
    "- **Dimensionality Reduction**: PCA for visualization\n",
    "- **Feature Normalization**: Critical for distance-based clustering\n",
    "- **Cluster Profiling**: Understanding segment characteristics\n",
    "\n",
    "#### Business Applications\n",
    "- **Marketing**: Segment-specific campaigns\n",
    "- **Sales**: Targeted account management\n",
    "- **Inventory**: Demand-based stocking\n",
    "- **Pricing**: Segment-based strategies\n",
    "- **Service**: Tiered service levels\n",
    "\n",
    "### Clustering Metrics Explained\n",
    "\n",
    "**Silhouette Score** (0-1, higher better):\n",
    "- Measures how similar a point is to its own cluster vs other clusters\n",
    "- >0.7: Strong structure\n",
    "- 0.5-0.7: Reasonable structure\n",
    "- <0.5: Weak or artificial structure\n",
    "\n",
    "**Calinski-Harabasz Index** (higher better):\n",
    "- Ratio of between-cluster to within-cluster variance\n",
    "- Higher = more distinct, well-separated clusters\n",
    "\n",
    "**Davies-Bouldin Index** (lower better):\n",
    "- Average similarity between each cluster and its most similar cluster\n",
    "- Lower = better separation\n",
    "\n",
    "### Business Value Achieved\n",
    "\n",
    "1. **Customer Understanding**:\n",
    "   - Identified distinct customer types\n",
    "   - Understood purchasing patterns\n",
    "   - Revealed segment opportunities\n",
    "\n",
    "2. **Operational Efficiency**:\n",
    "   - Optimize inventory by segment\n",
    "   - Target marketing efforts\n",
    "   - Personalize service delivery\n",
    "\n",
    "3. **Revenue Growth**:\n",
    "   - Cross-sell/upsell opportunities\n",
    "   - Segment-specific pricing\n",
    "   - Reduce customer churn\n",
    "\n",
    "### Limitations and Considerations\n",
    "\n",
    "1. **Current Limitations**:\n",
    "   - Only spending data (no demographics, geography details)\n",
    "   - Single time snapshot (no temporal patterns)\n",
    "   - Assumes stable segments over time\n",
    "\n",
    "2. **Important Notes**:\n",
    "   - Segments may overlap (fuzzy boundaries)\n",
    "   - Customers can move between segments\n",
    "   - Regular re-clustering needed\n",
    "   - Business context essential for naming\n",
    "\n",
    "3. **Future Improvements**:\n",
    "   - Add temporal data (seasonality, trends)\n",
    "   - Include customer demographics\n",
    "   - Geographic segmentation\n",
    "   - Predictive segment migration\n",
    "\n",
    "### Comparison with Classification\n",
    "\n",
    "**When to Use Clustering**:\n",
    "- No predefined categories\n",
    "- Exploratory data analysis\n",
    "- Discovering hidden patterns\n",
    "- Customer segmentation\n",
    "\n",
    "**When to Use Classification**:\n",
    "- Have labeled data\n",
    "- Predicting known outcomes\n",
    "- Clear target variable\n",
    "- Disease diagnosis, fraud detection\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [PyCaret Clustering Tutorial](https://pycaret.gitbook.io/docs/get-started/tutorials/clustering)\n",
    "- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [Customer Segmentation Best Practices](https://www.kaggle.com/learn)\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Bala Anbalagan  \n",
    "**Date**: January 2025  \n",
    "**Dataset**: [Kaggle - Wholesale Customers Dataset](https://www.kaggle.com/binovi/wholesale-customers-data-set)  \n",
    "**Original**: UCI Machine Learning Repository  \n",
    "**License**: MIT  \n",
    "\n",
    "---\n",
    "\n",
    "## Thank you for following this clustering tutorial!\n",
    "\n",
    "**Key Achievement**: We discovered meaningful customer segments from unlabeled data using unsupervised learning!\n",
    "\n",
    "**Main Insight**: Different customer types exist with distinct purchasing patterns - restaurants vs retail vs specialized customers.\n",
    "\n",
    "**Next Steps**:\n",
    "- Apply to your own customer data\n",
    "- Experiment with different K values\n",
    "- Implement segment-specific strategies\n",
    "\n",
    "**Disclaimer**: This is for educational purposes. Real-world segmentation should include additional customer data, domain expertise, and business validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}