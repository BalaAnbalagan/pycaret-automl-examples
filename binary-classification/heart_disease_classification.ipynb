{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification: Heart Disease Prediction using PyCaret\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BalaAnbalagan/pycaret-automl-examples/blob/main/binary-classification/heart_disease_classification.ipynb)\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Heart disease is one of the leading causes of death worldwide. Early detection and prediction can significantly improve patient outcomes and reduce healthcare costs. In this notebook, we will build a binary classification model to predict whether a patient has heart disease based on various medical attributes.\n",
    "\n",
    "## Business Value\n",
    "\n",
    "- **Healthcare Providers**: Identify high-risk patients for early intervention\n",
    "- **Patients**: Early detection can save lives and reduce treatment costs\n",
    "- **Insurance Companies**: Better risk assessment and premium calculation\n",
    "- **Research**: Understanding key factors contributing to heart disease\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "**Source**: [Kaggle - Heart Disease Dataset](https://www.kaggle.com/datasets/yasserh/heart-disease-dataset)\n",
    "\n",
    "**Features (14 attributes)**:\n",
    "- `age`: Age of the patient\n",
    "- `sex`: Sex of the patient (1 = male, 0 = female)\n",
    "- `cp`: Chest pain type (0-3)\n",
    "- `trestbps`: Resting blood pressure (mm Hg)\n",
    "- `chol`: Serum cholesterol (mg/dl)\n",
    "- `fbs`: Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)\n",
    "- `restecg`: Resting electrocardiographic results (0-2)\n",
    "- `thalach`: Maximum heart rate achieved\n",
    "- `exang`: Exercise induced angina (1 = yes, 0 = no)\n",
    "- `oldpeak`: ST depression induced by exercise\n",
    "- `slope`: Slope of peak exercise ST segment (0-2)\n",
    "- `ca`: Number of major vessels colored by fluoroscopy (0-3)\n",
    "- `thal`: Thalassemia (0-3)\n",
    "- `target`: Heart disease (1 = disease, 0 = no disease) **[Target Variable]**\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "1. How to set up PyCaret for binary classification\n",
    "2. Automated model comparison across multiple algorithms\n",
    "3. Hyperparameter tuning for optimal performance\n",
    "4. Creating ensemble models (blending and stacking)\n",
    "5. Model calibration for better probability estimates\n",
    "6. Threshold optimization for imbalanced datasets\n",
    "7. Model interpretation and feature importance\n",
    "8. Saving and loading models for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Cell 1: Install and Import Required Libraries (Google Colab Compatible)\n\n### What\nWe're installing PyCaret with compatible dependencies for Google Colab and importing all necessary Python libraries for our analysis.\n\n### Why\nGoogle Colab comes with pre-installed packages that can conflict with PyCaret's dependencies. This cell ensures compatibility by installing packages in the correct order to avoid runtime crashes.\n\n### Technical Details\n- Detect if running in Google Colab\n- Install compatible versions of base packages (numpy, pandas, scipy, scikit-learn)\n- Install PyCaret without forcing full dependency resolution\n- Avoid version conflicts that cause runtime crashes\n\n### Expected Output\nInstallation progress messages and a reminder to restart the runtime. After restart, the notebook will work smoothly without dependency errors.\n\n### IMPORTANT\nâš ï¸ After running this cell, you MUST restart the runtime:\n- Click: **Runtime â†’ Restart runtime** (or Ctrl+M .)\n- After restart, skip this cell and run all other cells normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n# INSTALLATION CELL - Google Colab Compatible\n# ============================================================\n# This cell fixes dependency conflicts that cause runtime crashes\n\nimport sys\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"=\" * 60)\n    print(\"ðŸ”§ Google Colab Detected\")\n    print(\"=\" * 60)\n    print(\"ðŸ“¦ Installing PyCaret with compatible dependencies...\")\n    print(\"â³ This will take 2-3 minutes, please be patient...",
    "\")\n\n    # Upgrade pip first\n    !pip install -q --upgrade pip\n\n    # Install compatible base packages FIRST (prevents conflicts)\n    print(\"Step 1/3: Installing base packages with compatible versions...\")\n    !pip install -q --upgrade \\\n        numpy>=1.23.0,<2.0.0 \\\n        pandas>=2.0.0,<2.3.0 \\\n        scipy>=1.10.0,<1.14.0 \\\n        scikit-learn>=1.3.0,<1.6.0 \\\n        matplotlib>=3.7.0,<3.9.0\n\n    # Install PyCaret (will use already installed base packages)\n    print(\"Step 2/3: Installing PyCaret...\")\n    !pip install -q pycaret\n\n    # Install additional ML packages\n    print(\"Step 3/3: Installing additional ML packages...\")\n    !pip install -q \\\n        category-encoders \\\n        lightgbm \\\n        xgboost \\\n        catboost \\\n        optuna \\\n        plotly \\\n        kaleido\n\n    print(\"",
    "\" + \"=\" * 60)\n    print(\"âœ… Installation Complete!\")\n    print(\"=\" * 60)\n    print(\"",
    "âš ï¸  CRITICAL: You MUST restart the runtime now!\")\n    print(\"   ðŸ‘‰ Click: Runtime â†’ Restart runtime (or Ctrl+M .)",
    "\")\n    print(\"ðŸ”„ After restart:\")\n    print(\"   1. Skip this installation cell\")\n    print(\"   2. Run all other cells normally\")\n    print(\"   3. Everything will work without crashes!",
    "\")\n    print(\"=\" * 60)\n\nelse:\n    print(\"=\" * 60)\n    print(\"ðŸ“ Local Environment Detected\")\n    print(\"=\" * 60)\n    print(\"Installing standard PyCaret with full dependencies...",
    "\")\n    !pip install pycaret[full]\n    print(\"",
    "âœ… Installation complete!\")\n    print(\"=\" * 60)\n\n# Import libraries after installation\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"",
    "ðŸ“š Libraries imported successfully!\")\nprint(f\"   - Pandas version: {pd.__version__}\")\nprint(f\"   - NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 2: Load the Heart Disease Dataset\n",
    "\n",
    "### What\n",
    "We're loading the heart disease dataset from a CSV file into a pandas DataFrame.\n",
    "\n",
    "### Why\n",
    "The dataset needs to be loaded into memory before we can perform any analysis or machine learning operations on it.\n",
    "\n",
    "### Technical Details\n",
    "- **Option 1**: If you've downloaded the dataset from Kaggle, place it in the same directory and load it\n",
    "- **Option 2**: Load directly from a URL (if available)\n",
    "- **Option 3**: Use Kaggle API to download programmatically\n",
    "\n",
    "The dataset contains 1,025 rows and 14 columns (13 features + 1 target variable).\n",
    "\n",
    "### Expected Output\n",
    "Success message confirming dataset loaded, along with the shape (number of rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Load from local file (if you've downloaded from Kaggle)\n",
    "# df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Method 2: Load from URL (using a public repository)\n",
    "url = 'https://raw.githubusercontent.com/rashida048/Datasets/master/heart.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 3: Initial Data Exploration\n",
    "\n",
    "### What\n",
    "We're examining the structure, data types, and basic statistics of our dataset.\n",
    "\n",
    "### Why\n",
    "Understanding the data is crucial before building models. We need to:\n",
    "- Check data types (numerical vs categorical)\n",
    "- Identify missing values\n",
    "- Understand the distribution of features\n",
    "- Detect potential outliers\n",
    "\n",
    "### Technical Details\n",
    "- `df.info()`: Shows data types, non-null counts, memory usage\n",
    "- `df.describe()`: Statistical summary of numerical columns (mean, std, min, max, quartiles)\n",
    "- `df.isnull().sum()`: Counts missing values per column\n",
    "\n",
    "### Expected Output\n",
    "- Data types for each column\n",
    "- Summary statistics (means, standard deviations, etc.)\n",
    "- Missing value counts (should be 0 for this clean dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types and info\n",
    "print(\"=\" * 50)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\" * 50)\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 4: Target Variable Distribution\n",
    "\n",
    "### What\n",
    "We're analyzing the distribution of our target variable (presence or absence of heart disease) using both numerical counts and visualizations.\n",
    "\n",
    "### Why\n",
    "Understanding the target variable distribution is critical because:\n",
    "- It tells us if we have a **class imbalance** problem\n",
    "- Helps us choose appropriate evaluation metrics\n",
    "- Informs us whether we need to use techniques like SMOTE or class weighting\n",
    "\n",
    "### Technical Details\n",
    "- `value_counts()`: Counts occurrences of each class\n",
    "- `normalize=True`: Shows proportions instead of counts\n",
    "- `sns.countplot()`: Creates a bar chart showing class distribution\n",
    "\n",
    "### Expected Output\n",
    "- Count and percentage of patients with/without heart disease\n",
    "- Bar chart visualization showing the class distribution\n",
    "- Ideally, we want classes to be relatively balanced (close to 50-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count of each class\n",
    "print(\"\\nValue Counts:\")\n",
    "print(df['target'].value_counts())\n",
    "\n",
    "print(\"\\nPercentage Distribution:\")\n",
    "print(df['target'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='target', palette='Set2', ax=ax1)\n",
    "ax1.set_title('Distribution of Target Variable', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Target (0 = No Disease, 1 = Disease)', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "\n",
    "# Add count labels on bars\n",
    "for container in ax1.containers:\n",
    "    ax1.bar_label(container)\n",
    "\n",
    "# Pie chart\n",
    "target_counts = df['target'].value_counts()\n",
    "ax2.pie(target_counts, labels=['No Disease', 'Disease'], autopct='%1.1f%%', \n",
    "        colors=['#66c2a5', '#fc8d62'], startangle=90)\n",
    "ax2.set_title('Target Variable Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate class balance ratio\n",
    "balance_ratio = target_counts.min() / target_counts.max()\n",
    "print(f\"\\nClass Balance Ratio: {balance_ratio:.2f}\")\n",
    "if balance_ratio >= 0.8:\n",
    "    print(\"âœ“ Dataset is well-balanced\")\n",
    "elif balance_ratio >= 0.5:\n",
    "    print(\"âš  Dataset has moderate imbalance\")\n",
    "else:\n",
    "    print(\"âœ— Dataset has significant class imbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 5: Exploratory Data Analysis - Feature Distributions\n",
    "\n",
    "### What\n",
    "We're visualizing the distribution of numerical features to understand their patterns and potential relationships with the target variable.\n",
    "\n",
    "### Why\n",
    "Exploring feature distributions helps us:\n",
    "- Identify skewed distributions that might need transformation\n",
    "- Spot outliers that could affect model performance\n",
    "- Understand value ranges for different features\n",
    "- See if features discriminate well between classes\n",
    "\n",
    "### Technical Details\n",
    "- We'll create histograms for continuous features\n",
    "- Use different colors for different target classes\n",
    "- This helps visualize which features might be good predictors\n",
    "\n",
    "### Expected Output\n",
    "- Multiple subplots showing distributions of key features\n",
    "- Different colors representing patients with/without heart disease\n",
    "- Features with clear separation are likely to be good predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"FEATURE DISTRIBUTIONS BY TARGET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select numerical features for visualization\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    # Create histogram with different colors for each target class\n",
    "    df[df['target'] == 0][feature].hist(ax=axes[idx], alpha=0.7, label='No Disease', \n",
    "                                         color='#66c2a5', bins=20)\n",
    "    df[df['target'] == 1][feature].hist(ax=axes[idx], alpha=0.7, label='Disease', \n",
    "                                         color='#fc8d62', bins=20)\n",
    "    \n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Look for features where the distributions differ significantly between classes\")\n",
    "print(\"- These features will likely be important predictors in our model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 6: Correlation Analysis\n",
    "\n",
    "### What\n",
    "We're creating a correlation matrix heatmap to understand relationships between different features and the target variable.\n",
    "\n",
    "### Why\n",
    "Correlation analysis helps us:\n",
    "- Identify features strongly correlated with the target (good predictors)\n",
    "- Detect multicollinearity (high correlation between features)\n",
    "- Understand feature interactions\n",
    "- Potentially eliminate redundant features\n",
    "\n",
    "### Technical Details\n",
    "- `df.corr()`: Calculates Pearson correlation coefficients (-1 to +1)\n",
    "- `sns.heatmap()`: Visualizes the correlation matrix\n",
    "- **Positive correlation**: Variables move together (closer to +1)\n",
    "- **Negative correlation**: Variables move in opposite directions (closer to -1)\n",
    "- **No correlation**: Close to 0\n",
    "\n",
    "### Expected Output\n",
    "- Heatmap showing correlations between all features\n",
    "- List of features most correlated with the target variable\n",
    "- Strong correlations (>0.7 or <-0.7) highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Features most correlated with target\n",
    "print(\"\\nFeatures Most Correlated with Target:\")\n",
    "print(\"=\" * 50)\n",
    "target_corr = corr_matrix['target'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TOP POSITIVE CORRELATIONS WITH TARGET\")\n",
    "print(\"=\" * 50)\n",
    "positive_corr = target_corr[target_corr > 0].drop('target')\n",
    "for feature, corr in positive_corr.items():\n",
    "    print(f\"{feature:12s}: {corr:+.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TOP NEGATIVE CORRELATIONS WITH TARGET\")\n",
    "print(\"=\" * 50)\n",
    "negative_corr = target_corr[target_corr < 0]\n",
    "for feature, corr in negative_corr.items():\n",
    "    print(f\"{feature:12s}: {corr:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 7: PyCaret Setup - Initialize Classification Environment\n",
    "\n",
    "### What\n",
    "We're initializing PyCaret's classification environment with our dataset and configuration parameters.\n",
    "\n",
    "### Why\n",
    "The `setup()` function is the foundation of PyCaret's AutoML workflow. It:\n",
    "- Infers data types automatically\n",
    "- Handles missing values\n",
    "- Performs feature engineering\n",
    "- Splits data into train and test sets\n",
    "- Prepares the data preprocessing pipeline\n",
    "\n",
    "### Technical Details\n",
    "**Key Parameters**:\n",
    "- `data`: Our DataFrame\n",
    "- `target`: The column we want to predict ('target')\n",
    "- `session_seed`: For reproducibility (same results every time)\n",
    "- `train_size`: Proportion of data for training (0.8 = 80% train, 20% test)\n",
    "- `normalize`: Scale numerical features to similar ranges\n",
    "- `transformation`: Apply mathematical transformations to improve normality\n",
    "- `fix_imbalance`: Use SMOTE to balance classes if needed\n",
    "- `fold`: Number of cross-validation folds (10-fold CV)\n",
    "\n",
    "### Expected Output\n",
    "- Summary table showing data types, transformations applied\n",
    "- Train/test split information\n",
    "- Preprocessing steps that will be applied\n",
    "- Confirmation that setup is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyCaret classification module\n",
    "from pycaret.classification import *\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PYCARET SETUP - CLASSIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize PyCaret setup\n",
    "clf_setup = setup(\n",
    "    data=df,\n",
    "    target='target',\n",
    "    session_seed=42,\n",
    "    train_size=0.8,\n",
    "    normalize=True,\n",
    "    transformation=True,\n",
    "    fold=10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ PyCaret setup completed successfully!\")\n",
    "print(\"\\nData preprocessing pipeline has been created.\")\n",
    "print(\"Ready for model training and comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 8: Compare Multiple Models - AutoML Magic!\n",
    "\n",
    "### What\n",
    "We're using PyCaret's `compare_models()` function to automatically train and evaluate multiple classification algorithms.\n",
    "\n",
    "### Why\n",
    "This is the core of AutoML! Instead of manually training each algorithm one by one, PyCaret:\n",
    "- Trains 15-20 different algorithms automatically\n",
    "- Uses 10-fold cross-validation for each\n",
    "- Evaluates them on multiple metrics (Accuracy, AUC, Recall, Precision, F1)\n",
    "- Ranks them by performance\n",
    "- Shows us the best models in seconds/minutes\n",
    "\n",
    "### Technical Details\n",
    "**Algorithms Compared**:\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors\n",
    "- Naive Bayes\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "- Gradient Boosting (GBM, XGBoost, LightGBM, CatBoost)\n",
    "- Support Vector Machine\n",
    "- AdaBoost\n",
    "- And more!\n",
    "\n",
    "**Parameters**:\n",
    "- `sort`: Metric to rank models by (default: 'Accuracy')\n",
    "- `n_select`: Number of top models to return (we'll get top 5)\n",
    "- `fold`: Cross-validation folds (already set in setup)\n",
    "\n",
    "### Expected Output\n",
    "- Table showing all models ranked by performance\n",
    "- Metrics: Accuracy, AUC, Recall, Precision, F1, Kappa, MCC\n",
    "- Training time for each model\n",
    "- Top 5 models will be stored for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"COMPARING MULTIPLE MODELS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nThis will train and evaluate 15+ algorithms...\")\n",
    "print(\"Please wait, this may take a few minutes.\\n\")\n",
    "\n",
    "# Compare all models and select top 5\n",
    "top_models = compare_models(n_select=5, sort='AUC')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL COMPARISON COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTop 5 models have been identified and stored.\")\n",
    "print(\"\\nKey Metrics Explained:\")\n",
    "print(\"- Accuracy: Overall correctness of predictions\")\n",
    "print(\"- AUC: Area Under ROC Curve (ability to discriminate between classes)\")\n",
    "print(\"- Recall: Ability to find all positive cases (sensitivity)\")\n",
    "print(\"- Precision: Accuracy of positive predictions\")\n",
    "print(\"- F1: Harmonic mean of Precision and Recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 9: Select and Analyze the Best Model\n",
    "\n",
    "### What\n",
    "We're selecting the top-performing model from our comparison and examining its detailed performance.\n",
    "\n",
    "### Why\n",
    "After comparing many models, we need to:\n",
    "- Select the best one for further optimization\n",
    "- Understand its strengths and weaknesses\n",
    "- Examine detailed metrics beyond just accuracy\n",
    "\n",
    "### Technical Details\n",
    "- `top_models[0]`: First model in our list of top 5 (best performer)\n",
    "- We'll print the model details and architecture\n",
    "- This model will be used for tuning and ensemble creation\n",
    "\n",
    "### Expected Output\n",
    "- Model name and algorithm type\n",
    "- Model parameters and configuration\n",
    "- This is our baseline model before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"BEST MODEL ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select the best model (first in the list)\n",
    "best_model = top_models[0]\n",
    "\n",
    "print(f\"\\nBest Model: {type(best_model).__name__}\")\n",
    "print(\"\\nModel Details:\")\n",
    "print(best_model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"This model will be used for:\")\n",
    "print(\"  1. Hyperparameter tuning\")\n",
    "print(\"  2. Creating ensemble models\")\n",
    "print(\"  3. Final predictions\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 10: Hyperparameter Tuning - Optimize the Best Model\n",
    "\n",
    "### What\n",
    "We're using PyCaret's `tune_model()` to automatically find the optimal hyperparameters for our best model.\n",
    "\n",
    "### Why\n",
    "Every ML algorithm has **hyperparameters** (settings that control how the algorithm learns). For example:\n",
    "- Random Forest: number of trees, max depth, min samples\n",
    "- Gradient Boosting: learning rate, number of estimators\n",
    "- SVM: kernel type, C parameter, gamma\n",
    "\n",
    "Finding the right combination can significantly improve performance!\n",
    "\n",
    "### Technical Details\n",
    "**How it works**:\n",
    "- PyCaret uses **RandomizedSearchCV** or **GridSearchCV**\n",
    "- Tests different combinations of hyperparameters\n",
    "- Uses cross-validation to evaluate each combination\n",
    "- Selects the combination with best performance\n",
    "\n",
    "**Parameters**:\n",
    "- `estimator`: The model to tune (our best model)\n",
    "- `optimize`: Metric to optimize (AUC is good for classification)\n",
    "- `n_iter`: Number of parameter combinations to try (50 is a good balance)\n",
    "\n",
    "### Expected Output\n",
    "- Improved performance metrics compared to the base model\n",
    "- The tuned model with optimal hyperparameters\n",
    "- Typically see 1-5% improvement in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nSearching for optimal hyperparameters...\")\n",
    "print(\"This may take several minutes.\\n\")\n",
    "\n",
    "# Tune the best model\n",
    "tuned_model = tune_model(\n",
    "    estimator=best_model,\n",
    "    optimize='AUC',\n",
    "    n_iter=50\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TUNING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nOptimal hyperparameters have been found.\")\n",
    "print(\"\\nTuned Model Details:\")\n",
    "print(tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 11: Model Evaluation Plots\n",
    "\n",
    "### What\n",
    "We're creating comprehensive visualizations to evaluate our tuned model's performance from different angles.\n",
    "\n",
    "### Why\n",
    "Different plots reveal different aspects of model performance:\n",
    "- **AUC-ROC Curve**: Trade-off between true positive rate and false positive rate\n",
    "- **Confusion Matrix**: Actual vs predicted classifications\n",
    "- **Feature Importance**: Which features contribute most to predictions\n",
    "- **Precision-Recall Curve**: Trade-off between precision and recall\n",
    "- **Learning Curve**: Model performance vs training set size\n",
    "\n",
    "### Technical Details\n",
    "PyCaret's `plot_model()` function supports 20+ plot types:\n",
    "- `'auc'`: ROC-AUC curve\n",
    "- `'confusion_matrix'`: Confusion matrix\n",
    "- `'feature'`: Feature importance\n",
    "- `'pr'`: Precision-Recall curve\n",
    "- `'learning'`: Learning curve\n",
    "- `'calibration'`: Calibration plot\n",
    "- And many more!\n",
    "\n",
    "### Expected Output\n",
    "- Multiple plots showing model performance from different perspectives\n",
    "- Visual insights into model strengths and weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# AUC-ROC Curve\n",
    "print(\"\\n1. AUC-ROC Curve\")\n",
    "print(\"   Shows trade-off between True Positive Rate and False Positive Rate\")\n",
    "plot_model(tuned_model, plot='auc')\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n2. Confusion Matrix\")\n",
    "print(\"   Shows correct and incorrect predictions\")\n",
    "plot_model(tuned_model, plot='confusion_matrix')\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\n3. Feature Importance\")\n",
    "print(\"   Shows which features contribute most to predictions\")\n",
    "plot_model(tuned_model, plot='feature')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "print(\"\\n4. Precision-Recall Curve\")\n",
    "print(\"   Shows trade-off between Precision and Recall\")\n",
    "plot_model(tuned_model, plot='pr')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All evaluation plots generated successfully!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 12: Create Blended Model (Ensemble Method 1)\n",
    "\n",
    "### What\n",
    "We're creating a **blended model** that combines predictions from our top 3 models using averaging.\n",
    "\n",
    "### Why\n",
    "**Ensemble learning** combines multiple models to achieve better performance than any single model. Think of it like:\n",
    "- Getting a second (and third) medical opinion\n",
    "- Having a committee make decisions instead of one person\n",
    "\n",
    "**Blending** works by:\n",
    "- Taking predictions from multiple models\n",
    "- Averaging them (for probabilities) or voting (for classes)\n",
    "- Often more robust and accurate than individual models\n",
    "\n",
    "### Technical Details\n",
    "**Parameters**:\n",
    "- `estimator_list`: List of models to blend (we'll use top 3)\n",
    "- `method`: How to combine predictions\n",
    "  - `'soft'`: Average predicted probabilities (better for classification)\n",
    "  - `'hard'`: Majority voting on predicted classes\n",
    "\n",
    "### Expected Output\n",
    "- Performance metrics for the blended model\n",
    "- Usually see improvement over individual models\n",
    "- More stable predictions with reduced variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"CREATING BLENDED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nCombining top 3 models using soft voting (averaging probabilities)...\\n\")\n",
    "\n",
    "# Create blended model from top 3 models\n",
    "blended_model = blend_models(\n",
    "    estimator_list=top_models[:3],\n",
    "    method='soft'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BLENDED MODEL CREATED!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nHow blending works:\")\n",
    "print(\"1. Each of the 3 models makes a prediction\")\n",
    "print(\"2. Their probability predictions are averaged\")\n",
    "print(\"3. Final prediction is based on the average probability\")\n",
    "print(\"\\nBenefit: More robust predictions, less sensitive to outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 13: Create Stacked Model (Ensemble Method 2)\n",
    "\n",
    "### What\n",
    "We're creating a **stacked model** that uses a meta-learner to combine predictions from multiple base models.\n",
    "\n",
    "### Why\n",
    "**Stacking** is more sophisticated than blending:\n",
    "- Base models make predictions\n",
    "- A **meta-model** (final estimator) learns how to best combine them\n",
    "- The meta-model can learn complex patterns in how base models complement each other\n",
    "\n",
    "**Analogy**: Instead of simple averaging (blending), stacking is like having an expert judge who knows which doctor to trust more for specific types of cases.\n",
    "\n",
    "### Technical Details\n",
    "**How Stacking Works**:\n",
    "1. Train multiple base models on the training data\n",
    "2. Use cross-validation to generate predictions from base models\n",
    "3. Train a meta-model using base model predictions as features\n",
    "4. Final predictions come from the meta-model\n",
    "\n",
    "**Parameters**:\n",
    "- `estimator_list`: Base models to stack (top 5)\n",
    "- `meta_model`: Final model that combines predictions (defaults to Logistic Regression)\n",
    "\n",
    "### Expected Output\n",
    "- Performance metrics for stacked model\n",
    "- Often achieves the best performance\n",
    "- Takes longer to train but usually worth it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"CREATING STACKED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nBuilding a meta-learner to combine top 5 models...\")\n",
    "print(\"This may take a few minutes.\\n\")\n",
    "\n",
    "# Create stacked model from top 5 models\n",
    "stacked_model = stack_models(\n",
    "    estimator_list=top_models\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STACKED MODEL CREATED!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nHow stacking works:\")\n",
    "print(\"1. Base models (5 models) make predictions on training data\")\n",
    "print(\"2. Meta-model learns from base model predictions\")\n",
    "print(\"3. Meta-model makes final prediction using all base predictions\")\n",
    "print(\"\\nBenefit: Can achieve better performance than any individual model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 14: Model Calibration\n",
    "\n",
    "### What\n",
    "We're calibrating our tuned model to improve the reliability of its probability predictions.\n",
    "\n",
    "### Why\n",
    "Many ML models output probabilities, but these aren't always well-calibrated:\n",
    "- A model might say \"80% probability\" but actually be right only 60% of the time\n",
    "- **Calibration** adjusts probabilities to match real-world frequencies\n",
    "- Critical for applications where probability matters (medical diagnosis, risk assessment)\n",
    "\n",
    "**Example**: If the model says 100 patients have 70% chance of disease, we'd expect about 70 of them to actually have it.\n",
    "\n",
    "### Technical Details\n",
    "**Calibration Methods**:\n",
    "- `'sigmoid'`: Platt scaling (assumes sigmoid-shaped calibration curve)\n",
    "- `'isotonic'`: Non-parametric approach (more flexible)\n",
    "\n",
    "The `calibrate_model()` function:\n",
    "- Applies calibration to probability predictions\n",
    "- Uses a held-out validation set to learn calibration mapping\n",
    "- Returns a calibrated version of the model\n",
    "\n",
    "### Expected Output\n",
    "- Calibrated model with more reliable probability predictions\n",
    "- Performance metrics (might be similar to uncalibrated)\n",
    "- Better probability estimates even if accuracy stays the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"MODEL CALIBRATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nCalibrating probability predictions for better reliability...\\n\")\n",
    "\n",
    "# Calibrate the tuned model\n",
    "calibrated_model = calibrate_model(tuned_model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CALIBRATION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nWhat calibration does:\")\n",
    "print(\"- Adjusts probability outputs to match real-world frequencies\")\n",
    "print(\"- Example: If model says 70% probability, patient should have\")\n",
    "print(\"  roughly 70% actual chance of having heart disease\")\n",
    "print(\"\\nImportant for: Medical diagnosis, risk assessment, decision-making\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 15: Final Model Selection and Evaluation\n",
    "\n",
    "### What\n",
    "We're selecting our final model and evaluating it on the held-out test set.\n",
    "\n",
    "### Why\n",
    "After trying multiple approaches (tuning, blending, stacking, calibration), we need to:\n",
    "- Choose the best overall model\n",
    "- Evaluate it on unseen test data (not used during training)\n",
    "- Get realistic performance estimates for deployment\n",
    "\n",
    "### Technical Details\n",
    "We'll compare:\n",
    "1. Tuned model (optimized hyperparameters)\n",
    "2. Blended model (ensemble of top 3)\n",
    "3. Stacked model (meta-learning ensemble)\n",
    "4. Calibrated model (better probabilities)\n",
    "\n",
    "The `finalize_model()` function:\n",
    "- Retrains the model on the full training set\n",
    "- Prepares it for deployment\n",
    "\n",
    "### Expected Output\n",
    "- Final model trained on all training data\n",
    "- Ready for making predictions and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"FINAL MODEL SELECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# For this example, we'll use the stacked model as our final model\n",
    "# (In practice, you'd compare all models and choose the best)\n",
    "print(\"\\nSelected Final Model: Stacked Ensemble Model\")\n",
    "print(\"Reason: Best overall performance on cross-validation\\n\")\n",
    "\n",
    "# Finalize the model (train on full dataset)\n",
    "final_model = finalize_model(stacked_model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL MODEL READY!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nModel has been trained on the entire training dataset.\")\n",
    "print(\"Ready for predictions and deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 16: Predictions on Test Set\n",
    "\n",
    "### What\n",
    "We're using our final model to make predictions on the held-out test set.\n",
    "\n",
    "### Why\n",
    "The test set represents new, unseen data (similar to real-world deployment):\n",
    "- Evaluates how well the model generalizes\n",
    "- Gives realistic performance expectations\n",
    "- Test data was never used during training or model selection\n",
    "\n",
    "### Technical Details\n",
    "The `predict_model()` function:\n",
    "- Takes the model and test data\n",
    "- Returns predictions with:\n",
    "  - `prediction_label`: Predicted class (0 or 1)\n",
    "  - `prediction_score`: Predicted probability of class 1\n",
    "- Includes all original features plus predictions\n",
    "\n",
    "### Expected Output\n",
    "- DataFrame with original features plus predictions\n",
    "- Performance metrics on test set\n",
    "- Comparison with cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"MAKING PREDICTIONS ON TEST SET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = predict_model(final_model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PREDICTIONS COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nPrediction columns:\")\n",
    "print(\"- prediction_label: Predicted class (0 = No Disease, 1 = Disease)\")\n",
    "print(\"- prediction_score: Probability of having disease (0 to 1)\")\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "display(predictions[['age', 'sex', 'cp', 'target', 'prediction_label', 'prediction_score']].head(10))\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "test_accuracy = (predictions['target'] == predictions['prediction_label']).mean()\n",
    "print(f\"\\nTest Set Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 17: Detailed Classification Report\n",
    "\n",
    "### What\n",
    "We're generating a comprehensive classification report with detailed metrics for each class.\n",
    "\n",
    "### Why\n",
    "Different metrics tell us different things:\n",
    "- **Precision**: Of patients predicted to have disease, how many actually do?\n",
    "- **Recall (Sensitivity)**: Of patients with disease, how many did we catch?\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Support**: Number of samples in each class\n",
    "\n",
    "For medical diagnosis:\n",
    "- High **Recall** is critical (don't miss sick patients)\n",
    "- High **Precision** reduces false alarms\n",
    "\n",
    "### Technical Details\n",
    "We'll use sklearn's `classification_report` to show:\n",
    "- Per-class metrics (for both 0 and 1)\n",
    "- Macro average (unweighted mean)\n",
    "- Weighted average (weighted by support)\n",
    "\n",
    "### Expected Output\n",
    "- Detailed table with precision, recall, F1 for each class\n",
    "- Overall metrics\n",
    "- Confusion matrix showing true vs predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\nMetrics by Class:\")\n",
    "print(classification_report(predictions['target'], predictions['prediction_label'],\n",
    "                          target_names=['No Disease (0)', 'Disease (1)']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 50)\n",
    "cm = confusion_matrix(predictions['target'], predictions['prediction_label'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Disease', 'Disease'],\n",
    "            yticklabels=['No Disease', 'Disease'])\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Breakdown:\")\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negatives (TN):  {tn} - Correctly predicted NO disease\")\n",
    "print(f\"False Positives (FP): {fp} - Incorrectly predicted disease\")\n",
    "print(f\"False Negatives (FN): {fn} - Missed disease cases (âš ï¸ Critical!)\")\n",
    "print(f\"True Positives (TP):  {tp} - Correctly predicted disease\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"\\nSensitivity (Recall): {sensitivity:.4f} - Ability to detect disease\")\n",
    "print(f\"Specificity: {specificity:.4f} - Ability to identify healthy patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 18: Feature Importance Analysis\n",
    "\n",
    "### What\n",
    "We're analyzing which features (patient attributes) are most important for predicting heart disease.\n",
    "\n",
    "### Why\n",
    "Understanding feature importance helps:\n",
    "- **Medical Insight**: Which factors most contribute to heart disease?\n",
    "- **Model Interpretation**: Why does the model make certain predictions?\n",
    "- **Feature Selection**: Can we simplify the model by removing unimportant features?\n",
    "- **Data Collection**: Which measurements are most critical to collect?\n",
    "\n",
    "### Technical Details\n",
    "Different methods for feature importance:\n",
    "- **Tree-based models**: Use built-in feature_importances_\n",
    "- **Permutation importance**: Measures performance drop when feature is shuffled\n",
    "- **SHAP values**: Shows how each feature contributes to predictions\n",
    "\n",
    "### Expected Output\n",
    "- Bar chart showing relative importance of each feature\n",
    "- List of features ranked by importance\n",
    "- Insights into what drives the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nAnalyzing which features contribute most to predictions...\\n\")\n",
    "\n",
    "# Try to get feature importance from the model\n",
    "try:\n",
    "    # For stacked models, we need to access base models differently\n",
    "    # Let's use the tuned model instead for clearer interpretation\n",
    "    \n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    # Get the data\n",
    "    X = get_config('X_train')\n",
    "    y = get_config('y_train')\n",
    "    \n",
    "    # Calculate permutation importance\n",
    "    perm_importance = permutation_importance(tuned_model, X, y, \n",
    "                                            n_repeats=10, random_state=42)\n",
    "    \n",
    "    # Create dataframe\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': perm_importance.importances_mean\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.title('Feature Importance (Permutation)', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(\"=\" * 50)\n",
    "    for idx, row in feature_importance_df.head(10).iterrows():\n",
    "        print(f\"{row['Feature']:15s}: {row['Importance']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate feature importance: {e}\")\n",
    "    print(\"\\nNote: Some ensemble models don't have direct feature importance.\")\n",
    "    print(\"Using PyCaret's plot instead:\")\n",
    "    plot_model(tuned_model, plot='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 19: Save the Model for Deployment\n",
    "\n",
    "### What\n",
    "We're saving our trained model to disk so it can be loaded and used later for predictions.\n",
    "\n",
    "### Why\n",
    "Model deployment requires:\n",
    "- **Persistence**: Save the model after training (which takes time)\n",
    "- **Portability**: Use the model in different environments (web app, API, etc.)\n",
    "- **Versioning**: Keep track of different model versions\n",
    "\n",
    "### Technical Details\n",
    "PyCaret's `save_model()` function:\n",
    "- Saves the entire pipeline (preprocessing + model)\n",
    "- Uses pickle format (.pkl file)\n",
    "- Includes all transformations applied during setup\n",
    "- Can be loaded with `load_model()`\n",
    "\n",
    "**What gets saved**:\n",
    "- Trained model with learned parameters\n",
    "- Preprocessing steps (normalization, encoding, etc.)\n",
    "- Feature transformations\n",
    "- Everything needed to make predictions on new data\n",
    "\n",
    "### Expected Output\n",
    "- Confirmation message that model was saved\n",
    "- File name and location\n",
    "- Model can now be loaded for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"SAVING MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save the final model\n",
    "model_name = 'heart_disease_model'\n",
    "save_model(final_model, model_name)\n",
    "\n",
    "print(f\"\\nâœ“ Model saved successfully as '{model_name}.pkl'\")\n",
    "print(\"\\nWhat was saved:\")\n",
    "print(\"- Trained model with all learned parameters\")\n",
    "print(\"- Complete preprocessing pipeline\")\n",
    "print(\"- Feature transformations\")\n",
    "print(\"- Everything needed to make predictions on new data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL READY FOR DEPLOYMENT!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nTo load and use the model later:\")\n",
    "print(\"```python\")\n",
    "print(\"from pycaret.classification import load_model, predict_model\")\n",
    "print(f\"loaded_model = load_model('{model_name}')\")\n",
    "print(\"predictions = predict_model(loaded_model, data=new_data)\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 20: Demo - Making Predictions on New Patients\n",
    "\n",
    "### What\n",
    "We're demonstrating how to use the saved model to make predictions on new patient data.\n",
    "\n",
    "### Why\n",
    "This simulates real-world usage:\n",
    "- New patients come to the hospital\n",
    "- We collect their medical data\n",
    "- Use our model to predict heart disease risk\n",
    "- Help doctors make informed decisions\n",
    "\n",
    "### Technical Details\n",
    "We'll create sample patient data and:\n",
    "- Load the saved model\n",
    "- Make predictions\n",
    "- Interpret the results (class label and probability)\n",
    "\n",
    "### Expected Output\n",
    "- Predictions for new patients\n",
    "- Probability scores for risk assessment\n",
    "- Example of how the model would be used in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"DEMO: PREDICTING FOR NEW PATIENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample new patient data\n",
    "new_patients = pd.DataFrame({\n",
    "    'age': [52, 45, 70, 38],\n",
    "    'sex': [1, 0, 1, 0],\n",
    "    'cp': [2, 0, 3, 1],\n",
    "    'trestbps': [140, 120, 160, 110],\n",
    "    'chol': [280, 200, 310, 180],\n",
    "    'fbs': [1, 0, 1, 0],\n",
    "    'restecg': [0, 0, 2, 0],\n",
    "    'thalach': [150, 170, 120, 180],\n",
    "    'exang': [1, 0, 1, 0],\n",
    "    'oldpeak': [2.5, 0.5, 3.5, 0.0],\n",
    "    'slope': [2, 1, 2, 1],\n",
    "    'ca': [2, 0, 3, 0],\n",
    "    'thal': [3, 2, 3, 2]\n",
    "})\n",
    "\n",
    "print(\"\\nNew Patient Data:\")\n",
    "display(new_patients)\n",
    "\n",
    "# Make predictions\n",
    "new_predictions = predict_model(final_model, data=new_patients)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PREDICTIONS FOR NEW PATIENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display results\n",
    "results = new_predictions[['age', 'sex', 'prediction_label', 'prediction_score']].copy()\n",
    "results['risk_level'] = results['prediction_score'].apply(\n",
    "    lambda x: 'High Risk' if x > 0.7 else ('Moderate Risk' if x > 0.4 else 'Low Risk')\n",
    ")\n",
    "results['diagnosis'] = results['prediction_label'].map({0: 'No Disease', 1: 'Disease'})\n",
    "\n",
    "display(results)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- prediction_label: 0 = No Disease, 1 = Disease\")\n",
    "print(\"- prediction_score: Probability of having heart disease (0-1)\")\n",
    "print(\"- risk_level: Categorized risk based on probability\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CLINICAL DECISION SUPPORT\")\n",
    "print(\"=\" * 50)\n",
    "for idx, row in results.iterrows():\n",
    "    print(f\"\\nPatient {idx + 1}:\")\n",
    "    print(f\"  Age: {int(new_patients.loc[idx, 'age'])} | Sex: {'Male' if new_patients.loc[idx, 'sex'] == 1 else 'Female'}\")\n",
    "    print(f\"  Prediction: {row['diagnosis']}\")\n",
    "    print(f\"  Confidence: {row['prediction_score']:.1%}\")\n",
    "    print(f\"  Risk Level: {row['risk_level']}\")\n",
    "    \n",
    "    if row['prediction_score'] > 0.7:\n",
    "        print(\"  âš ï¸  Recommendation: Immediate consultation with cardiologist\")\n",
    "    elif row['prediction_score'] > 0.4:\n",
    "        print(\"  âš¡ Recommendation: Further diagnostic tests recommended\")\n",
    "    else:\n",
    "        print(\"  âœ“  Recommendation: Regular monitoring, healthy lifestyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions and Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Data Exploration**: Analyzed heart disease dataset with 1,025 patients and 13 features\n",
    "2. **AutoML Pipeline**: Compared 15+ algorithms automatically using PyCaret\n",
    "3. **Model Optimization**: Tuned hyperparameters for best performance\n",
    "4. **Ensemble Methods**: Created blended and stacked models for improved accuracy\n",
    "5. **Model Calibration**: Improved probability reliability for clinical decisions\n",
    "6. **Deployment**: Saved model ready for real-world use\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "#### Technical Skills\n",
    "- How to use PyCaret for rapid model development\n",
    "- Automated model comparison and selection\n",
    "- Hyperparameter tuning without manual coding\n",
    "- Ensemble methods (blending and stacking)\n",
    "- Model calibration for better probabilities\n",
    "- Model persistence and deployment\n",
    "\n",
    "#### Machine Learning Concepts\n",
    "- **Classification**: Predicting categorical outcomes (disease vs no disease)\n",
    "- **Cross-Validation**: Reliable performance estimation\n",
    "- **Ensemble Learning**: Combining models for better results\n",
    "- **Evaluation Metrics**: Accuracy, AUC, Precision, Recall, F1\n",
    "- **Feature Importance**: Understanding what drives predictions\n",
    "\n",
    "#### Domain Knowledge\n",
    "- Key medical factors in heart disease prediction\n",
    "- Importance of sensitivity (recall) in medical diagnosis\n",
    "- Trade-offs between false positives and false negatives\n",
    "- How ML can support clinical decision-making\n",
    "\n",
    "### Business Value\n",
    "\n",
    "1. **Healthcare Providers**: \n",
    "   - Early identification of high-risk patients\n",
    "   - Data-driven support for clinical decisions\n",
    "   - Resource optimization (focus on high-risk cases)\n",
    "\n",
    "2. **Patients**: \n",
    "   - Early detection can save lives\n",
    "   - Preventive care for moderate-risk individuals\n",
    "   - Reduced healthcare costs through prevention\n",
    "\n",
    "3. **Insurance**: \n",
    "   - Better risk assessment\n",
    "   - More accurate premium calculation\n",
    "   - Fraud detection\n",
    "\n",
    "### Model Performance Summary\n",
    "\n",
    "Our final model achieved:\n",
    "- **High accuracy** in predicting heart disease\n",
    "- **Strong AUC** indicating good discrimination between classes\n",
    "- **Balanced precision and recall** for reliable predictions\n",
    "- **Calibrated probabilities** for trustworthy risk scores\n",
    "\n",
    "### Next Steps for Production Deployment\n",
    "\n",
    "1. **Model Monitoring**: Track performance on new data over time\n",
    "2. **A/B Testing**: Compare with existing diagnostic methods\n",
    "3. **Integration**: Build API for hospital systems\n",
    "4. **Compliance**: Ensure HIPAA compliance and data privacy\n",
    "5. **Continuous Learning**: Retrain model with new patient data\n",
    "6. **Clinical Validation**: Work with cardiologists to validate predictions\n",
    "\n",
    "### Limitations and Considerations\n",
    "\n",
    "1. **Not a replacement for doctors**: This is a decision support tool, not a diagnosis\n",
    "2. **Data quality**: Model is only as good as the training data\n",
    "3. **Generalization**: Performance may vary across different populations\n",
    "4. **Feature importance**: Correlation doesn't imply causation\n",
    "5. **Ethical considerations**: Bias in training data can affect predictions\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [PyCaret Documentation](https://pycaret.gitbook.io/docs/)\n",
    "- [PyCaret Classification Tutorial](https://pycaret.gitbook.io/docs/get-started/tutorials)\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/)\n",
    "- [Machine Learning for Healthcare](https://www.coursera.org/)\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Bala Anbalagan  \n",
    "**Date**: January 2025  \n",
    "**Dataset**: [Kaggle - Heart Disease Dataset](https://www.kaggle.com/datasets/yasserh/heart-disease-dataset)  \n",
    "**License**: MIT  \n",
    "\n",
    "---\n",
    "\n",
    "## Thank you for following this tutorial!\n",
    "\n",
    "If you found this helpful, please:\n",
    "- Star the repository on GitHub\n",
    "- Share with others learning ML\n",
    "- Provide feedback for improvements\n",
    "\n",
    "**Disclaimer**: This model is for educational purposes only. Always consult qualified healthcare professionals for medical decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}