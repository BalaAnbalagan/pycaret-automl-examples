{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n\n## Cell 0: Setup Virtual Environment (Local Environment Only)\n\n### What\nWe're creating a dedicated virtual environment for this project to isolate dependencies and ensure reproducibility.\n\n### Why\nUsing a virtual environment is a best practice because:\n- Isolates project dependencies from system Python\n- Prevents version conflicts with other projects\n- Makes the project portable and reproducible\n- Allows specific package versions without affecting other projects\n\n### Technical Details\n**For Local Development**:\n1. Create a virtual environment using Python 3.9+\n2. Activate the virtual environment\n3. Install PyCaret with specific compatible versions\n\n**For Google Colab**: Skip this cell (Colab manages its own environment)\n\n### Instructions\n\n**Option 1: Using venv (recommended)**\n```bash\n# Navigate to your project directory\ncd /Users/banbalagan/Projects/pycaret-automl-examples\n\n# Create virtual environment\npython3.9 -m venv venv\n\n# Activate virtual environment\n# On macOS/Linux:\nsource venv/bin/activate\n# On Windows:\n# venv\\Scripts\\activate\n\n# Verify Python version\npython --version\n\n# Continue to next cell for package installation\n```\n\n**Option 2: Using conda**\n```bash\n# Create conda environment\nconda create -n pycaret-env python=3.9 -y\n\n# Activate environment\nconda activate pycaret-env\n```\n\n### Expected Output\nAfter activation, your terminal prompt should show `(venv)` or `(pycaret-env)` prefix, indicating the virtual environment is active.\n\n### Important Notes\n- Run this in your terminal BEFORE opening Jupyter Notebook\n- After creating/activating the virtual environment, install Jupyter in it:\n  ```bash\n  pip install jupyter notebook\n  ```\n- Then launch Jupyter from within the activated environment:\n  ```bash\n  jupyter notebook\n  ```\n- Select the kernel that corresponds to your virtual environment in Jupyter",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Cell 1: Install and Import Required Libraries\n\n### What\nWe're installing PyCaret with compatible dependencies and importing all necessary Python libraries for our analysis.\n\n### Why\nGoogle Colab comes with pre-installed packages that can conflict with PyCaret's dependencies. For local environments, we install specific package versions to ensure stability and reproducibility.\n\n### Technical Details\n- **Google Colab**: Install compatible versions to avoid runtime crashes\n- **Local Environment**: Install PyCaret with specific versions (Option 2 - recommended)\n- Import all necessary libraries for data analysis and machine learning\n\n### Expected Output\n- **Google Colab**: Installation messages and a reminder to restart the runtime\n- **Local Environment**: Clean installation of all required packages\n\n### IMPORTANT (Google Colab Users)\nâš ï¸ After running this cell in Colab, you MUST restart the runtime:\n- Click: **Runtime â†’ Restart runtime** (or Ctrl+M .)\n- After restart, skip this cell and run all other cells normally"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================\n# INSTALLATION CELL - Environment Detection & Package Setup\n# ============================================================\n\nimport sys\nimport os\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"=\" * 60)\n    print(\"ðŸ”§ Google Colab Detected\")\n    print(\"=\" * 60)\n    print(\"ðŸ“¦ Installing PyCaret with compatible dependencies...\")\n    print(\"â³ This will take 2-3 minutes, please be patient...\")\n\n    # Upgrade pip first\n    !pip install -q --upgrade pip\n\n    # Install compatible base packages FIRST (prevents conflicts)\n    print(\"Step 1/3: Installing base packages with compatible versions...\")\n    !pip install -q --upgrade \\\n        numpy>=1.23.0,<2.0.0 \\\n        pandas>=2.0.0,<2.3.0 \\\n        scipy>=1.10.0,<1.14.0 \\\n        scikit-learn>=1.3.0,<1.6.0 \\\n        matplotlib>=3.7.0,<3.9.0\n\n    # Install PyCaret (will use already installed base packages)\n    print(\"Step 2/3: Installing PyCaret...\")\n    !pip install -q pycaret\n\n    # Install additional ML packages\n    print(\"Step 3/3: Installing additional ML packages...\")\n    !pip install -q \\\n        category-encoders \\\n        lightgbm \\\n        xgboost \\\n        catboost \\\n        optuna \\\n        plotly \\\n        kaleido\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"âœ… Installation Complete!\")\n    print(\"=\" * 60)\n    print(\"âš ï¸  CRITICAL: You MUST restart the runtime now!\")\n    print(\"   ðŸ‘‰ Click: Runtime â†’ Restart runtime (or Ctrl+M .)\")\n    print(\"ðŸ”„ After restart:\")\n    print(\"   1. Skip this installation cell\")\n    print(\"   2. Run all other cells normally\")\n    print(\"   3. Everything will work without crashes!\")\n    print(\"=\" * 60)\n\nelse:\n    print(\"=\" * 60)\n    print(\"ðŸ“ Local Environment Detected\")\n    print(\"=\" * 60)\n    print(\"Installing PyCaret with specific compatible versions...\")\n    print(\"Using Option 2: Controlled dependency installation\\n\")\n    \n    # Upgrade pip first\n    !pip install -q --upgrade pip\n    \n    # Install base packages with specific versions (Option 2)\n    print(\"Step 1/4: Installing base packages...\")\n    !pip install -q \\\n        numpy>=1.23.0,<2.0.0 \\\n        pandas>=2.0.0,<2.3.0 \\\n        scipy>=1.10.0,<1.14.0 \\\n        scikit-learn>=1.3.0,<1.6.0 \\\n        matplotlib>=3.7.0,<3.9.0 \\\n        seaborn>=0.12.0\n    \n    # Install PyCaret\n    print(\"Step 2/4: Installing PyCaret...\")\n    !pip install -q pycaret\n    \n    # Install additional ML packages\n    print(\"Step 3/4: Installing additional ML libraries...\")\n    !pip install -q \\\n        category-encoders \\\n        lightgbm \\\n        xgboost \\\n        catboost \\\n        optuna \\\n        plotly \\\n        kaleido\n    \n    # Install notebook support packages\n    print(\"Step 4/4: Installing notebook support packages...\")\n    !pip install -q ipywidgets\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"âœ… Installation Complete!\")\n    print(\"=\" * 60)\n    print(\"All packages installed successfully in your virtual environment.\")\n    print(\"You can now proceed with running the rest of the notebook.\")\n    print(\"=\" * 60)\n\n# Import libraries after installation\nprint(\"\\nðŸ“š Importing libraries...\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set visualization style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nprint(\"\\nâœ“ Libraries imported successfully!\")\nprint(f\"   - Python version: {sys.version.split()[0]}\")\nprint(f\"   - Pandas version: {pd.__version__}\")\nprint(f\"   - NumPy version: {np.__version__}\")\nprint(f\"   - Working directory: {os.getcwd()}\")\n\n# Check if running in virtual environment\nif hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):\n    print(f\"   - Virtual environment: Active âœ“\")\n    print(f\"   - Environment path: {sys.prefix}\")\nelse:\n    print(\"   - Virtual environment: Not detected (consider using venv)\")\n    \nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## Cell 2: Load Network Traffic Dataset\n\n### What\nLoading network traffic data for anomaly detection.\n\n### Why\nFor demonstration, we'll create synthetic network traffic data with normal and anomalous patterns. In production:\n- Real-time network packet capture\n- Feature extraction from flow data\n- Continuous monitoring\n\n### Technical Details\n- **Local Environment**: Can load from local PCAP/CSV files if available\n- **Google Colab & Demo**: Creates synthetic network traffic data\n- Network traffic features capture flow characteristics that can reveal attacks\n\n### Dataset Locations\n- **Local Path**: `/Users/banbalagan/Projects/pycaret-automl-examples/datasets/anomaly-detection/BCCC-CIC-IDS2017/BCCC-CIC-IDS-2017/` (multiple CSV files)\n- **Demo Mode**: Synthetic data generation for educational purposes\n\n### Expected Output\nDataset loaded with network flow features."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import sys\nimport os\nimport glob\n\n# Check if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\n\n# Define dataset paths\nLOCAL_DIR = '/Users/banbalagan/Projects/pycaret-automl-examples/datasets/anomaly-detection/BCCC-CIC-IDS2017/BCCC-CIC-IDS-2017/'\n\n# Try to load from local directory first (for local development)\nif not IN_COLAB and os.path.exists(LOCAL_DIR):\n    print(\"=\" * 60)\n    print(\"ðŸ“‚ Loading dataset from local directory...\")\n    print(\"=\" * 60)\n    print(f\"Path: {LOCAL_DIR}\\n\")\n    \n    # Find all CSV files in the directory\n    csv_files = glob.glob(os.path.join(LOCAL_DIR, '*.csv'))\n    \n    if csv_files:\n        print(f\"Found {len(csv_files)} CSV file(s)\")\n        print(\"\\nLoading first file as sample...\")\n        # Load first file as sample (these files can be large)\n        df = pd.read_csv(csv_files[0], nrows=1000)  # Load first 1000 rows for demo\n        print(f\"âœ“ Dataset loaded successfully from: {os.path.basename(csv_files[0])}\")\n        print(f\"  (Loaded first 1000 rows for demonstration)\")\n        use_synthetic = False\n    else:\n        print(\"âš ï¸  No CSV files found in directory.\")\n        print(\"Creating synthetic data for demonstration...\\n\")\n        use_synthetic = True\n        \nelif not IN_COLAB:\n    # Local environment but directory doesn't exist\n    print(\"=\" * 60)\n    print(\"âš ï¸  Local directory not found...\")\n    print(\"=\" * 60)\n    print(f\"Expected location: {LOCAL_DIR}\")\n    print(\"\\nCreating synthetic network traffic data for demonstration...\\n\")\n    use_synthetic = True\n    \nelse:\n    # Google Colab - use synthetic data\n    print(\"=\" * 60)\n    print(\"â˜ï¸  Google Colab Detected\")\n    print(\"=\" * 60)\n    print(\"Creating synthetic network traffic data for demonstration...\\n\")\n    use_synthetic = True\n\n# Create synthetic data if needed\nif use_synthetic or not 'df' in locals():\n    np.random.seed(42)\n    \n    # Generate normal network traffic (90%)\n    n_normal = 900\n    normal_data = {\n        'flow_duration': np.random.normal(120000, 30000, n_normal),\n        'total_fwd_packets': np.random.poisson(50, n_normal),\n        'total_bwd_packets': np.random.poisson(45, n_normal),\n        'flow_bytes_per_sec': np.random.normal(5000, 1000, n_normal),\n        'flow_packets_per_sec': np.random.normal(100, 20, n_normal),\n        'fwd_header_length': np.random.normal(200, 50, n_normal),\n        'bwd_header_length': np.random.normal(180, 40, n_normal)\n    }\n    \n    # Generate anomalous traffic (10% - attacks, scans, etc.)\n    n_anomaly = 100\n    anomaly_data = {\n        'flow_duration': np.random.normal(5000, 2000, n_anomaly),  # Very short\n        'total_fwd_packets': np.random.poisson(200, n_anomaly),  # Unusually high\n        'total_bwd_packets': np.random.poisson(5, n_anomaly),  # Unusually low\n        'flow_bytes_per_sec': np.random.normal(50000, 10000, n_anomaly),  # Very high\n        'flow_packets_per_sec': np.random.normal(500, 100, n_anomaly),  # Very high\n        'fwd_header_length': np.random.normal(400, 100, n_anomaly),  # High\n        'bwd_header_length': np.random.normal(50, 20, n_anomaly)  # Low\n    }\n    \n    # Combine and create DataFrame\n    df_normal = pd.DataFrame(normal_data)\n    df_anomaly = pd.DataFrame(anomaly_data)\n    df = pd.concat([df_normal, df_anomaly], ignore_index=True)\n    \n    # Shuffle\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    # Add true labels (for evaluation only - not used in training)\n    true_labels = [0]*n_normal + [1]*n_anomaly\n    df['true_anomaly'] = true_labels\n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    print(\"âœ“ Synthetic network traffic dataset created!\")\n\n# Display basic information\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DATASET INFORMATION\")\nprint(\"=\" * 60)\nprint(f\"Shape: {df.shape[0]} network flows, {df.shape[1]} features\")\nprint(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\nif 'true_anomaly' in df.columns:\n    print(f\"\\nTrue distribution (for evaluation):\")\n    print(f\"- Normal traffic: {(df['true_anomaly']==0).sum()}\")\n    print(f\"- Anomalous traffic: {(df['true_anomaly']==1).sum()}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FIRST 5 ROWS\")\nprint(\"=\" * 60)\ndf.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 3: Exploratory Data Analysis\n",
    "\n",
    "### What\n",
    "Exploring network traffic patterns to understand normal vs anomalous behavior.\n",
    "\n",
    "### Why\n",
    "Understanding data helps:\n",
    "- Identify features that distinguish anomalies\n",
    "- Set realistic expectations\n",
    "- Guide algorithm selection\n",
    "\n",
    "### Technical Details\n",
    "We'll visualize distributions to see if anomalies are visually distinct.\n",
    "\n",
    "### Expected Output\n",
    "Statistical summary and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"NETWORK TRAFFIC ANALYSIS\")\nprint(\"=\" * 60)\n\n# Remove true_anomaly for unsupervised analysis (if present in synthetic data)\nif 'true_anomaly' in df.columns:\n    features_df = df.drop('true_anomaly', axis=1)\n    print(\"â„¹ï¸  Using synthetic data with ground truth labels\")\nelse:\n    features_df = df.copy()\n    print(\"â„¹ï¸  Using real network traffic data (no ground truth)\")\n\nprint(\"\\nStatistical Summary:\")\ndisplay(features_df.describe())\n\n# Visualize distributions\nfig, axes = plt.subplots(2, 4, figsize=(18, 10))\naxes = axes.ravel()\n\nfor idx, col in enumerate(features_df.columns):\n    axes[idx].hist(features_df[col], bins=50, alpha=0.7, edgecolor='black')\n    axes[idx].set_title(col, fontsize=10, fontweight='bold')\n    axes[idx].set_xlabel('Value')\n    axes[idx].set_ylabel('Frequency')\n    axes[idx].grid(alpha=0.3)\n\n# Remove extra subplot\nfig.delaxes(axes[7])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey Observations:\")\nprint(\"- Most traffic follows normal patterns (main distribution)\")\nprint(\"- Some outliers visible (potential anomalies)\")\nprint(\"- Different features show different spread\")\nprint(\"- Anomaly detection will flag unusual combinations\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 4: PyCaret Setup for Anomaly Detection\n",
    "\n",
    "### What\n",
    "Initializing PyCaret's anomaly detection environment.\n",
    "\n",
    "### Why\n",
    "Anomaly detection setup prepares data for:\n",
    "- Outlier identification\n",
    "- Anomaly scoring\n",
    "- Threshold-based flagging\n",
    "\n",
    "### Technical Details\n",
    "Like clustering, anomaly detection is unsupervised:\n",
    "- No target variable\n",
    "- No train/test split (use all data)\n",
    "- Normalization important\n",
    "\n",
    "### Expected Output\n",
    "Setup summary for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pycaret.anomaly import *\n\nprint(\"=\" * 60)\nprint(\"PYCARET SETUP - ANOMALY DETECTION\")\nprint(\"=\" * 60)\nprint(\"\\nConfiguring unsupervised anomaly detection...\\n\")\n\n# Setup (exclude true_anomaly from training)\n# Changed from session_seed to session_id for PyCaret 3.x\nanomaly_setup = setup(\n    data=features_df,\n    normalize=True,\n    session_id=42,\n    verbose=True\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"âœ“ Anomaly detection setup complete!\")\nprint(\"=\" * 60)\nprint(\"\\nKey Points:\")\nprint(\"- UNSUPERVISED: No labels used in training\")\nprint(\"- GOAL: Flag unusual network flows\")\nprint(\"- ASSUMPTION: Most traffic is normal, anomalies are rare\")\nprint(\"\\nReady to detect network intrusions!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 5: Create Anomaly Detection Models\n",
    "\n",
    "### What\n",
    "Creating multiple anomaly detection models with different algorithms.\n",
    "\n",
    "### Why\n",
    "Different algorithms detect different types of anomalies:\n",
    "\n",
    "**Isolation Forest**:\n",
    "- Fast and scalable\n",
    "- Isolates anomalies using random trees\n",
    "- Good for high-dimensional data\n",
    "\n",
    "**LOF (Local Outlier Factor)**:\n",
    "- Density-based detection\n",
    "- Compares local density to neighbors\n",
    "- Good for varying density clusters\n",
    "\n",
    "**One-Class SVM**:\n",
    "- Learns boundary around normal data\n",
    "- Flags points outside boundary\n",
    "- Good for well-defined normal region\n",
    "\n",
    "### Technical Details\n",
    "Each algorithm assigns anomaly scores and labels.\n",
    "\n",
    "### Expected Output\n",
    "Models created with anomaly detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATING ANOMALY DETECTION MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Isolation Forest\n",
    "print(\"\\n1. Isolation Forest\")\n",
    "print(\"   - Fast, scalable algorithm\")\n",
    "print(\"   - Isolates anomalies using random trees\")\n",
    "iforest = create_model('iforest', fraction=0.1)\n",
    "\n",
    "# LOF (Local Outlier Factor)\n",
    "print(\"\\n2. Local Outlier Factor (LOF)\")\n",
    "print(\"   - Density-based detection\")\n",
    "print(\"   - Compares local density to neighbors\")\n",
    "lof = create_model('lof', fraction=0.1)\n",
    "\n",
    "# One-Class SVM\n",
    "print(\"\\n3. One-Class SVM\")\n",
    "print(\"   - Learns boundary around normal data\")\n",
    "print(\"   - Flags outliers outside boundary\")\n",
    "ocsvm = create_model('svm', fraction=0.1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"All models created!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNote: 'fraction=0.1' means expect ~10% anomalies\")\n",
    "print(\"Adjust based on your network's typical anomaly rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 6: Assign Anomaly Labels and Scores\n",
    "\n",
    "### What\n",
    "Assigning anomaly labels (0=normal, 1=anomaly) and scores to each network flow.\n",
    "\n",
    "### Why\n",
    "Anomaly detection output includes:\n",
    "- **Binary label**: Normal (0) or Anomaly (1)\n",
    "- **Anomaly score**: Continuous score (higher = more anomalous)\n",
    "- **Decision function**: Distance from normal region\n",
    "\n",
    "### Technical Details\n",
    "Scores allow ranking:\n",
    "- Investigate highest-scoring flows first\n",
    "- Adjust thresholds based on resources\n",
    "- Create priority alerts\n",
    "\n",
    "### Expected Output\n",
    "Dataset with anomaly predictions and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"ASSIGNING ANOMALY LABELS AND SCORES\")\nprint(\"=\" * 60)\n\n# Assign predictions (using Isolation Forest)\npredictions = assign_model(iforest)\n\n# Add true labels for evaluation (if we have them)\nif 'true_anomaly' in df.columns:\n    predictions['true_anomaly'] = df['true_anomaly'].values\n    has_ground_truth = True\nelse:\n    has_ground_truth = False\n\nprint(\"\\nPredictions completed!\")\nprint(f\"\\nColumns added:\")\nprint(\"- Anomaly: Binary label (0=Normal, 1=Anomaly)\")\nprint(\"- Anomaly_Score: Continuous score (higher = more suspicious)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DETECTION RESULTS\")\nprint(\"=\" * 60)\nprint(f\"\\nTotal flows analyzed: {len(predictions)}\")\nprint(f\"Flagged as anomalies: {(predictions['Anomaly']==1).sum()}\")\nprint(f\"Marked as normal: {(predictions['Anomaly']==0).sum()}\")\n\nprint(\"\\nSample predictions (sorted by anomaly score):\")\nif has_ground_truth:\n    display(predictions[['flow_duration', 'total_fwd_packets', 'Anomaly', 'Anomaly_Score', 'true_anomaly']]\n            .sort_values('Anomaly_Score', ascending=False).head(10))\nelse:\n    display(predictions[['flow_duration', 'total_fwd_packets', 'Anomaly', 'Anomaly_Score']]\n            .sort_values('Anomaly_Score', ascending=False).head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 7: Evaluate Detection Performance\n",
    "\n",
    "### What\n",
    "Evaluating how well our model detects true anomalies (since we have ground truth for this demo).\n",
    "\n",
    "### Why\n",
    "In production, we don't have labels, but for this demo:\n",
    "- Check if model catches real attacks\n",
    "- Understand false positive rate\n",
    "- Validate approach before deployment\n",
    "\n",
    "### Technical Details\n",
    "Key metrics:\n",
    "- **Precision**: Of flagged flows, how many are truly anomalous?\n",
    "- **Recall**: Of true anomalies, how many did we catch?\n",
    "- **F1-Score**: Balance between precision and recall\n",
    "\n",
    "### Expected Output\n",
    "Confusion matrix and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import classification_report, confusion_matrix\n\nprint(\"=\" * 60)\nprint(\"ANOMALY DETECTION PERFORMANCE\")\nprint(\"=\" * 60)\n\n# Check if we have ground truth labels\nif 'true_anomaly' in predictions.columns:\n    print(\"\\nNote: In production, we don't have true labels.\")\n    print(\"This evaluation is possible because we created synthetic data.\\n\")\n\n    # Classification report\n    print(classification_report(predictions['true_anomaly'], predictions['Anomaly'],\n                              target_names=['Normal', 'Anomaly']))\n\n    # Confusion matrix\n    cm = confusion_matrix(predictions['true_anomaly'], predictions['Anomaly'])\n\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Normal', 'Anomaly'],\n                yticklabels=['Normal', 'Anomaly'])\n    plt.title('Confusion Matrix - Anomaly Detection', fontsize=14, fontweight='bold')\n    plt.ylabel('True Label', fontsize=12)\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CONFUSION MATRIX BREAKDOWN\")\n    print(\"=\" * 60)\n    tn, fp, fn, tp = cm.ravel()\n    print(f\"\\nTrue Negatives (TN):  {tn} - Correctly identified normal traffic\")\n    print(f\"False Positives (FP): {fp} - False alarms (normal flagged as anomaly)\")\n    print(f\"False Negatives (FN): {fn} - Missed attacks (anomaly marked as normal) âš ï¸\")\n    print(f\"True Positives (TP):  {tp} - Correctly caught anomalies âœ“\")\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CYBERSECURITY PERSPECTIVE\")\n    print(\"=\" * 60)\n    print(\"\\n- False Positives (FP): Alert fatigue, wasted investigation time\")\n    print(\"- False Negatives (FN): Missed threats - MOST CRITICAL!\")\n    print(\"\\nTrade-off: Lower threshold = More FP but fewer FN (catch more attacks)\")\nelse:\n    print(\"\\nâš ï¸  No ground truth labels available (real network traffic data)\")\n    print(\"=\" * 60)\n    print(\"IN PRODUCTION MODE\")\n    print(\"=\" * 60)\n    print(\"\\nWithout labeled data, we focus on:\")\n    print(\"1. Anomaly score distribution analysis\")\n    print(\"2. Manual investigation of top-scoring flows\")\n    print(\"3. Feedback loop to improve detection over time\")\n    print(\"4. Integration with security monitoring systems\")\n    \n    # Show anomaly score distribution\n    plt.figure(figsize=(10, 6))\n    plt.hist(predictions['Anomaly_Score'], bins=50, alpha=0.7, edgecolor='black', color='steelblue')\n    plt.axvline(predictions['Anomaly_Score'].median(), color='red', linestyle='--', \n                label=f'Median: {predictions[\"Anomaly_Score\"].median():.3f}')\n    plt.title('Distribution of Anomaly Scores', fontsize=14, fontweight='bold')\n    plt.xlabel('Anomaly Score', fontsize=12)\n    plt.ylabel('Frequency', fontsize=12)\n    plt.legend()\n    plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"NEXT STEPS FOR INVESTIGATION\")\n    print(\"=\" * 60)\n    print(\"\\n1. Review top 10 highest-scoring flows manually\")\n    print(\"2. Correlate with firewall/IDS logs\")\n    print(\"3. Check for known attack signatures\")\n    print(\"4. Adjust threshold based on investigation capacity\")\n    print(\"5. Create feedback loop to retrain model\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 8: Visualize Anomalies\n",
    "\n",
    "### What\n",
    "Visualizing detected anomalies in 2D space using dimensionality reduction.\n",
    "\n",
    "### Why\n",
    "Helps understand:\n",
    "- Where anomalies lie in feature space\n",
    "- If they form patterns\n",
    "- Model behavior\n",
    "\n",
    "### Technical Details\n",
    "PCA or t-SNE reduces dimensions for visualization.\n",
    "\n",
    "### Expected Output\n",
    "2D plot showing normal vs anomalous flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VISUALIZING ANOMALIES IN 2D\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# PyCaret's built-in visualization\n",
    "plot_model(iforest, plot='tsne')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTERPRETING THE VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n- Blue points: Normal network traffic\")\n",
    "print(\"- Red/Yellow points: Detected anomalies\")\n",
    "print(\"- t-SNE reduces 7 dimensions to 2D\")\n",
    "print(\"\\nGood detection shows:\")\n",
    "print(\"- Anomalies at edges/outside main cluster\")\n",
    "print(\"- Clear separation from normal traffic\")\n",
    "print(\"- Some clustering of similar attack types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 9: Analyze Top Anomalies\n",
    "\n",
    "### What\n",
    "Examining the most suspicious network flows for investigation.\n",
    "\n",
    "### Why\n",
    "In production:\n",
    "- Security teams investigate top-scoring flows first\n",
    "- Limited resources require prioritization\n",
    "- Understanding patterns helps create rules\n",
    "\n",
    "### Technical Details\n",
    "Sort by anomaly score to get most suspicious flows.\n",
    "\n",
    "### Expected Output\n",
    "List of highest-scoring anomalies with characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"TOP 10 MOST SUSPICIOUS NETWORK FLOWS\")\nprint(\"=\" * 60)\n\n# Get top anomalies\ntop_anomalies = predictions[predictions['Anomaly']==1].sort_values('Anomaly_Score', ascending=False).head(10)\n\nprint(\"\\nThese flows should be investigated first:\\n\")\nif 'true_anomaly' in predictions.columns:\n    display(top_anomalies[['flow_duration', 'total_fwd_packets', 'total_bwd_packets',\n                           'flow_bytes_per_sec', 'Anomaly_Score', 'true_anomaly']])\nelse:\n    display(top_anomalies[['flow_duration', 'total_fwd_packets', 'total_bwd_packets',\n                           'flow_bytes_per_sec', 'Anomaly_Score']])\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"CHARACTERISTICS OF DETECTED ANOMALIES\")\nprint(\"=\" * 60)\n\n# Prepare data for comparison (remove prediction columns)\ncols_to_drop = ['Anomaly', 'Anomaly_Score']\nif 'true_anomaly' in predictions.columns:\n    cols_to_drop.append('true_anomaly')\n\nanomalies_only = predictions[predictions['Anomaly']==1].drop(cols_to_drop, axis=1)\nnormal_only = predictions[predictions['Anomaly']==0].drop(cols_to_drop, axis=1)\n\ncomparison = pd.DataFrame({\n    'Feature': anomalies_only.columns,\n    'Normal_Mean': normal_only.mean().values,\n    'Anomaly_Mean': anomalies_only.mean().values\n})\ncomparison['Difference_%'] = ((comparison['Anomaly_Mean'] - comparison['Normal_Mean']) / \n                               comparison['Normal_Mean'] * 100).round(1)\n\nprint(\"\\nAverage values: Normal vs Anomalous traffic\")\ndisplay(comparison)\n\nprint(\"\\nKey patterns in anomalies:\")\nfor idx, row in comparison.iterrows():\n    if abs(row['Difference_%']) > 50:\n        direction = \"higher\" if row['Difference_%'] > 0 else \"lower\"\n        print(f\"- {row['Feature']}: {abs(row['Difference_%']):.1f}% {direction} than normal\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cell 10: Save Anomaly Detection Model\n",
    "\n",
    "### What\n",
    "Saving the trained anomaly detection model for deployment.\n",
    "\n",
    "### Why\n",
    "Production deployment:\n",
    "- Real-time network monitoring\n",
    "- Continuous threat detection\n",
    "- Automated alerting\n",
    "- Integration with SIEM systems\n",
    "\n",
    "### Technical Details\n",
    "Model can score new flows in real-time.\n",
    "\n",
    "### Expected Output\n",
    "Saved model ready for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAVING ANOMALY DETECTION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_name = 'network_intrusion_detector'\n",
    "save_model(iforest, model_name)\n",
    "\n",
    "print(f\"\\nâœ“ Model saved as '{model_name}.pkl'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEPLOYMENT ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n1. Network Traffic Capture\")\n",
    "print(\"   â†“ Extract flow features\")\n",
    "print(\"2. Feature Engineering\")\n",
    "print(\"   â†“ Normalize, transform\")\n",
    "print(\"3. Anomaly Detection Model\")\n",
    "print(\"   â†“ Score each flow\")\n",
    "print(\"4. Alert System\")\n",
    "print(\"   â†“ High scores trigger alerts\")\n",
    "print(\"5. Security Team Investigation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRODUCTION CONSIDERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n- Set appropriate anomaly threshold based on team capacity\")\n",
    "print(\"- Implement alert prioritization (score-based)\")\n",
    "print(\"- Regular model retraining with new normal traffic\")\n",
    "print(\"- Feedback loop: Confirmed attacks improve model\")\n",
    "print(\"- Integration with firewall for automatic blocking\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TO USE THE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n```python\")\n",
    "print(\"from pycaret.anomaly import load_model, predict_model\")\n",
    "print(f\"model = load_model('{model_name}')\")\n",
    "print(\"predictions = predict_model(model, data=new_traffic)\")\n",
    "print(\"suspicious = predictions[predictions['Anomaly']==1]\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions and Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Anomaly Detection**: Identified unusual network traffic patterns\n",
    "2. **Unsupervised Learning**: No labeled attacks needed for training\n",
    "3. **Multiple Algorithms**: Compared Isolation Forest, LOF, One-Class SVM\n",
    "4. **Anomaly Scoring**: Ranked suspicious flows for investigation\n",
    "5. **Production Ready**: Model ready for real-time intrusion detection\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "#### Anomaly Detection vs Other ML Tasks\n",
    "\n",
    "| Aspect | Classification | Clustering | Anomaly Detection |\n",
    "|--------|---------------|-----------|------------------|\n",
    "| Goal | Predict labels | Find groups | Find outliers |\n",
    "| Labels | Required | None | None |\n",
    "| Output | Class | Cluster ID | Anomaly score |\n",
    "| Assumption | Balanced classes | Natural groups | Most data is normal |\n",
    "| Use Case | Diagnosis | Segmentation | Fraud, intrusion |\n",
    "\n",
    "#### Technical Skills\n",
    "- **Isolation Forest**: Fast, scalable outlier detection\n",
    "- **LOF**: Density-based anomaly detection\n",
    "- **One-Class SVM**: Boundary-based detection\n",
    "- **Anomaly Scoring**: Continuous scores for prioritization\n",
    "- **Threshold Selection**: Balancing false positives vs false negatives\n",
    "\n",
    "#### Cybersecurity Applications\n",
    "- **Network Intrusion Detection**: Flag suspicious traffic\n",
    "- **Zero-Day Attacks**: Detect unknown threats\n",
    "- **Insider Threats**: Unusual user behavior\n",
    "- **DDoS Detection**: Abnormal traffic patterns\n",
    "- **Malware Detection**: Anomalous system behavior\n",
    "\n",
    "### Anomaly Detection Algorithms\n",
    "\n",
    "**Isolation Forest**:\n",
    "- **How**: Isolates anomalies using random trees\n",
    "- **Strengths**: Fast, scalable, handles high dimensions\n",
    "- **Best for**: Large datasets, real-time detection\n",
    "\n",
    "**Local Outlier Factor (LOF)**:\n",
    "- **How**: Compares local density to neighbors\n",
    "- **Strengths**: Good for varying density regions\n",
    "- **Best for**: Complex data with multiple normal patterns\n",
    "\n",
    "**One-Class SVM**:\n",
    "- **How**: Learns boundary around normal data\n",
    "- **Strengths**: Effective for well-defined normal region\n",
    "- **Best for**: High-dimensional data with clear normal zone\n",
    "\n",
    "### Business Value\n",
    "\n",
    "1. **Security**:\n",
    "   - Early threat detection\n",
    "   - Reduced dwell time\n",
    "   - Proactive defense\n",
    "\n",
    "2. **Cost Savings**:\n",
    "   - Prevent data breaches ($millions)\n",
    "   - Reduce manual monitoring\n",
    "   - Minimize downtime\n",
    "\n",
    "3. **Compliance**:\n",
    "   - Meet security monitoring requirements\n",
    "   - Audit trail of threats\n",
    "   - Demonstrate due diligence\n",
    "\n",
    "### Challenges and Considerations\n",
    "\n",
    "1. **False Positives**:\n",
    "   - Alert fatigue if too many\n",
    "   - Wasted investigation time\n",
    "   - Need to tune threshold\n",
    "\n",
    "2. **False Negatives**:\n",
    "   - Missed attacks are costly\n",
    "   - More dangerous than false positives\n",
    "   - Balance with other security layers\n",
    "\n",
    "3. **Concept Drift**:\n",
    "   - Normal traffic patterns change\n",
    "   - New attack types emerge\n",
    "   - Regular retraining needed\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "**Real-time Pipeline**:\n",
    "1. Capture network packets\n",
    "2. Extract flow features\n",
    "3. Normalize/preprocess\n",
    "4. Score with model\n",
    "5. Alert if anomaly\n",
    "6. Investigate and respond\n",
    "\n",
    "**Best Practices**:\n",
    "- Start with conservative threshold (fewer alerts)\n",
    "- Gradually tune based on investigation feedback\n",
    "- Implement tiered alerting (critical/medium/low)\n",
    "- Integrate with SIEM for correlation\n",
    "- Regular model updates with new normal traffic\n",
    "- Maintain human-in-the-loop for critical decisions\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Training Data Quality**:\n",
    "   - Assumes training data is mostly normal\n",
    "   - Contaminated training = poor detection\n",
    "\n",
    "2. **Novel Attacks**:\n",
    "   - Can miss sophisticated attacks that mimic normal traffic\n",
    "   - Should be one layer in defense-in-depth strategy\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Quality of features crucial\n",
    "   - Domain expertise needed\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "1. **Deep Learning**: Neural networks for complex patterns\n",
    "2. **Ensemble Methods**: Combine multiple detectors\n",
    "3. **Temporal Analysis**: Sequence-based detection\n",
    "4. **Contextual Features**: User, time, location context\n",
    "5. **Active Learning**: Feedback from investigations\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [PyCaret Anomaly Detection](https://pycaret.gitbook.io/docs/get-started/tutorials/anomaly-detection)\n",
    "- [Scikit-learn Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "- [Network Intrusion Detection](https://www.coursera.org/)\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Bala Anbalagan  \n",
    "**Date**: January 2025  \n",
    "**Dataset**: Synthetic network traffic (based on CIC-IDS-2017 patterns)  \n",
    "**License**: MIT  \n",
    "\n",
    "---\n",
    "\n",
    "## Thank you for following this anomaly detection tutorial!\n",
    "\n",
    "**Key Achievement**: Built an unsupervised intrusion detection system without labeled attacks!\n",
    "\n",
    "**Main Insight**: Anomaly detection excels at finding unusual patterns, making it perfect for cybersecurity where new threats constantly emerge.\n",
    "\n",
    "**Next Steps**:\n",
    "- Apply to real network traffic data\n",
    "- Integrate with SIEM systems\n",
    "- Deploy for continuous monitoring\n",
    "\n",
    "**Disclaimer**: For educational purposes. Production intrusion detection requires comprehensive security architecture, not just anomaly detection. Always combine with firewalls, IDS/IPS, endpoint protection, and security expertise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}